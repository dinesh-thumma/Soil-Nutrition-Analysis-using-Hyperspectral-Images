{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "54ijnEbB2VNw",
    "outputId": "ab3b4b0e-6c83-492b-c4f2-bac62115cbad"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics,model_selection,preprocessing,linear_model,ensemble,decomposition,tree\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error, roc_auc_score,f1_score,roc_curve,accuracy_score,r2_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler,MinMaxScaler,PolynomialFeatures\n",
    "from sklearn.preprocessing import OrdinalEncoder,LabelEncoder,OrdinalEncoder,OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, KFold ,GridSearchCV,StratifiedKFold,RepeatedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor,HistGradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor  # Import the MLPRegressor\n",
    "from catboost import CatBoostRegressor,CatBoostClassifier\n",
    "from lightgbm import LGBMRegressor,LGBMClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from xgboost import XGBRegressor,XGBClassifier\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import folium\n",
    "#!pip install geopy\n",
    "#from geopy.geocoders import Nominatim\n",
    "\n",
    "import gc,itertools, pickle, re,time,warnings\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "from random import choice, choices\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn2\n",
    "#pip install pandas-profiling\n",
    "#import pandas_profiling\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "from functools import reduce\n",
    "from tqdm import tqdm\n",
    "from itertools import cycle\n",
    "from matplotlib_venn import venn2\n",
    "\n",
    "from scipy import stats\n",
    "from functools import reduce\n",
    "seed = 777\n",
    "import random\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "plt.style.use(\"ggplot\")\n",
    "color_pal = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "\n",
    "def calculate_rmse(true_values, predicted_values):\n",
    "    mse = mean_squared_error(true_values, predicted_values)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "#from google.colab import files, drive\n",
    "#drive.mount('./content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "sJOjQeDERUXF"
   },
   "outputs": [],
   "source": [
    "# Load files\n",
    "#data_path = \"/content/content/MyDrive/DATA-ANALYSIS/ZINDI-COMPETITIONS/TABULAR/Digital Green Crop Yield Estimate Challeng/\"\n",
    "train = pd.read_csv('train_df.csv')\n",
    "test = pd.read_csv('test_df.csv')\n",
    "#sample_submission = pd.read_csv(data_path + 'SampleSubmission.csv')\n",
    "#var_desc = pd.read_csv(data_path + 'VariableDescription.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "szIX6SQH2VN2"
   },
   "outputs": [],
   "source": [
    "#var_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13856, 2402)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Value</th>\n",
       "      <th>feat1</th>\n",
       "      <th>feat2</th>\n",
       "      <th>feat3</th>\n",
       "      <th>feat4</th>\n",
       "      <th>feat5</th>\n",
       "      <th>feat6</th>\n",
       "      <th>feat7</th>\n",
       "      <th>feat8</th>\n",
       "      <th>feat9</th>\n",
       "      <th>feat10</th>\n",
       "      <th>feat11</th>\n",
       "      <th>feat12</th>\n",
       "      <th>feat13</th>\n",
       "      <th>feat14</th>\n",
       "      <th>feat15</th>\n",
       "      <th>feat16</th>\n",
       "      <th>feat17</th>\n",
       "      <th>feat18</th>\n",
       "      <th>feat19</th>\n",
       "      <th>feat20</th>\n",
       "      <th>feat21</th>\n",
       "      <th>feat22</th>\n",
       "      <th>feat23</th>\n",
       "      <th>feat24</th>\n",
       "      <th>feat25</th>\n",
       "      <th>feat26</th>\n",
       "      <th>feat27</th>\n",
       "      <th>feat28</th>\n",
       "      <th>feat29</th>\n",
       "      <th>feat30</th>\n",
       "      <th>feat31</th>\n",
       "      <th>feat32</th>\n",
       "      <th>feat33</th>\n",
       "      <th>feat34</th>\n",
       "      <th>feat35</th>\n",
       "      <th>feat36</th>\n",
       "      <th>feat37</th>\n",
       "      <th>feat38</th>\n",
       "      <th>feat39</th>\n",
       "      <th>feat40</th>\n",
       "      <th>feat41</th>\n",
       "      <th>feat42</th>\n",
       "      <th>feat43</th>\n",
       "      <th>feat44</th>\n",
       "      <th>feat45</th>\n",
       "      <th>feat46</th>\n",
       "      <th>feat47</th>\n",
       "      <th>feat48</th>\n",
       "      <th>...</th>\n",
       "      <th>feat2351</th>\n",
       "      <th>feat2352</th>\n",
       "      <th>feat2353</th>\n",
       "      <th>feat2354</th>\n",
       "      <th>feat2355</th>\n",
       "      <th>feat2356</th>\n",
       "      <th>feat2357</th>\n",
       "      <th>feat2358</th>\n",
       "      <th>feat2359</th>\n",
       "      <th>feat2360</th>\n",
       "      <th>feat2361</th>\n",
       "      <th>feat2362</th>\n",
       "      <th>feat2363</th>\n",
       "      <th>feat2364</th>\n",
       "      <th>feat2365</th>\n",
       "      <th>feat2366</th>\n",
       "      <th>feat2367</th>\n",
       "      <th>feat2368</th>\n",
       "      <th>feat2369</th>\n",
       "      <th>feat2370</th>\n",
       "      <th>feat2371</th>\n",
       "      <th>feat2372</th>\n",
       "      <th>feat2373</th>\n",
       "      <th>feat2374</th>\n",
       "      <th>feat2375</th>\n",
       "      <th>feat2376</th>\n",
       "      <th>feat2377</th>\n",
       "      <th>feat2378</th>\n",
       "      <th>feat2379</th>\n",
       "      <th>feat2380</th>\n",
       "      <th>feat2381</th>\n",
       "      <th>feat2382</th>\n",
       "      <th>feat2383</th>\n",
       "      <th>feat2384</th>\n",
       "      <th>feat2385</th>\n",
       "      <th>feat2386</th>\n",
       "      <th>feat2387</th>\n",
       "      <th>feat2388</th>\n",
       "      <th>feat2389</th>\n",
       "      <th>feat2390</th>\n",
       "      <th>feat2391</th>\n",
       "      <th>feat2392</th>\n",
       "      <th>feat2393</th>\n",
       "      <th>feat2394</th>\n",
       "      <th>feat2395</th>\n",
       "      <th>feat2396</th>\n",
       "      <th>feat2397</th>\n",
       "      <th>feat2398</th>\n",
       "      <th>feat2399</th>\n",
       "      <th>feat2400</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_P</td>\n",
       "      <td>45.1</td>\n",
       "      <td>0.203683</td>\n",
       "      <td>0.203992</td>\n",
       "      <td>0.202177</td>\n",
       "      <td>0.205636</td>\n",
       "      <td>0.210894</td>\n",
       "      <td>0.220505</td>\n",
       "      <td>0.226073</td>\n",
       "      <td>0.227012</td>\n",
       "      <td>0.228581</td>\n",
       "      <td>0.230678</td>\n",
       "      <td>0.233982</td>\n",
       "      <td>0.238588</td>\n",
       "      <td>0.241646</td>\n",
       "      <td>0.250014</td>\n",
       "      <td>0.258405</td>\n",
       "      <td>0.261899</td>\n",
       "      <td>0.266396</td>\n",
       "      <td>0.270737</td>\n",
       "      <td>0.274890</td>\n",
       "      <td>0.273968</td>\n",
       "      <td>0.276430</td>\n",
       "      <td>0.283327</td>\n",
       "      <td>0.295028</td>\n",
       "      <td>0.308494</td>\n",
       "      <td>0.316209</td>\n",
       "      <td>0.320717</td>\n",
       "      <td>0.327333</td>\n",
       "      <td>0.331691</td>\n",
       "      <td>0.337992</td>\n",
       "      <td>0.342740</td>\n",
       "      <td>0.349642</td>\n",
       "      <td>0.355668</td>\n",
       "      <td>0.360805</td>\n",
       "      <td>0.364883</td>\n",
       "      <td>0.367570</td>\n",
       "      <td>0.370319</td>\n",
       "      <td>0.376717</td>\n",
       "      <td>0.381299</td>\n",
       "      <td>0.387101</td>\n",
       "      <td>0.393264</td>\n",
       "      <td>0.399336</td>\n",
       "      <td>0.402669</td>\n",
       "      <td>0.405688</td>\n",
       "      <td>0.407629</td>\n",
       "      <td>0.411576</td>\n",
       "      <td>0.416467</td>\n",
       "      <td>0.419428</td>\n",
       "      <td>0.419526</td>\n",
       "      <td>...</td>\n",
       "      <td>1.080411</td>\n",
       "      <td>1.095705</td>\n",
       "      <td>1.129028</td>\n",
       "      <td>1.153727</td>\n",
       "      <td>1.191501</td>\n",
       "      <td>1.206518</td>\n",
       "      <td>1.240862</td>\n",
       "      <td>1.266370</td>\n",
       "      <td>1.281632</td>\n",
       "      <td>1.303487</td>\n",
       "      <td>1.320699</td>\n",
       "      <td>1.343543</td>\n",
       "      <td>1.349235</td>\n",
       "      <td>1.355927</td>\n",
       "      <td>1.355282</td>\n",
       "      <td>1.360762</td>\n",
       "      <td>1.357333</td>\n",
       "      <td>1.372652</td>\n",
       "      <td>1.387570</td>\n",
       "      <td>1.400359</td>\n",
       "      <td>0.574390</td>\n",
       "      <td>0.584549</td>\n",
       "      <td>0.670901</td>\n",
       "      <td>0.769619</td>\n",
       "      <td>0.920841</td>\n",
       "      <td>1.042641</td>\n",
       "      <td>1.146396</td>\n",
       "      <td>1.211927</td>\n",
       "      <td>1.273028</td>\n",
       "      <td>1.312649</td>\n",
       "      <td>1.513034</td>\n",
       "      <td>1.593172</td>\n",
       "      <td>1.678123</td>\n",
       "      <td>1.751818</td>\n",
       "      <td>1.816820</td>\n",
       "      <td>1.870275</td>\n",
       "      <td>1.911317</td>\n",
       "      <td>1.919118</td>\n",
       "      <td>1.924227</td>\n",
       "      <td>1.962802</td>\n",
       "      <td>0.805311</td>\n",
       "      <td>0.936756</td>\n",
       "      <td>1.290456</td>\n",
       "      <td>1.626534</td>\n",
       "      <td>1.777969</td>\n",
       "      <td>2.094892</td>\n",
       "      <td>2.382082</td>\n",
       "      <td>2.567942</td>\n",
       "      <td>2.697733</td>\n",
       "      <td>2.733829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_P</td>\n",
       "      <td>44.8</td>\n",
       "      <td>0.250603</td>\n",
       "      <td>0.249785</td>\n",
       "      <td>0.246786</td>\n",
       "      <td>0.250632</td>\n",
       "      <td>0.255035</td>\n",
       "      <td>0.265170</td>\n",
       "      <td>0.270217</td>\n",
       "      <td>0.269648</td>\n",
       "      <td>0.269498</td>\n",
       "      <td>0.270211</td>\n",
       "      <td>0.272503</td>\n",
       "      <td>0.276662</td>\n",
       "      <td>0.279905</td>\n",
       "      <td>0.288868</td>\n",
       "      <td>0.297204</td>\n",
       "      <td>0.301549</td>\n",
       "      <td>0.306996</td>\n",
       "      <td>0.313169</td>\n",
       "      <td>0.319590</td>\n",
       "      <td>0.320913</td>\n",
       "      <td>0.326546</td>\n",
       "      <td>0.336216</td>\n",
       "      <td>0.350023</td>\n",
       "      <td>0.364886</td>\n",
       "      <td>0.372914</td>\n",
       "      <td>0.377352</td>\n",
       "      <td>0.383670</td>\n",
       "      <td>0.387081</td>\n",
       "      <td>0.392847</td>\n",
       "      <td>0.396461</td>\n",
       "      <td>0.401595</td>\n",
       "      <td>0.405064</td>\n",
       "      <td>0.406857</td>\n",
       "      <td>0.406138</td>\n",
       "      <td>0.403846</td>\n",
       "      <td>0.402338</td>\n",
       "      <td>0.404513</td>\n",
       "      <td>0.405366</td>\n",
       "      <td>0.408354</td>\n",
       "      <td>0.412235</td>\n",
       "      <td>0.416498</td>\n",
       "      <td>0.418540</td>\n",
       "      <td>0.420223</td>\n",
       "      <td>0.421406</td>\n",
       "      <td>0.424962</td>\n",
       "      <td>0.428652</td>\n",
       "      <td>0.430207</td>\n",
       "      <td>0.429708</td>\n",
       "      <td>...</td>\n",
       "      <td>1.256190</td>\n",
       "      <td>1.502146</td>\n",
       "      <td>1.758616</td>\n",
       "      <td>2.011209</td>\n",
       "      <td>2.160844</td>\n",
       "      <td>2.194579</td>\n",
       "      <td>2.249652</td>\n",
       "      <td>2.285842</td>\n",
       "      <td>2.307155</td>\n",
       "      <td>2.331671</td>\n",
       "      <td>2.351600</td>\n",
       "      <td>2.378405</td>\n",
       "      <td>2.375424</td>\n",
       "      <td>2.375780</td>\n",
       "      <td>2.363581</td>\n",
       "      <td>2.404702</td>\n",
       "      <td>2.419109</td>\n",
       "      <td>2.419744</td>\n",
       "      <td>2.425575</td>\n",
       "      <td>2.429126</td>\n",
       "      <td>0.706865</td>\n",
       "      <td>0.714469</td>\n",
       "      <td>0.781618</td>\n",
       "      <td>0.895926</td>\n",
       "      <td>1.084361</td>\n",
       "      <td>1.143641</td>\n",
       "      <td>1.187382</td>\n",
       "      <td>1.231635</td>\n",
       "      <td>1.280067</td>\n",
       "      <td>1.232515</td>\n",
       "      <td>1.738577</td>\n",
       "      <td>2.511333</td>\n",
       "      <td>3.038777</td>\n",
       "      <td>3.175525</td>\n",
       "      <td>3.269756</td>\n",
       "      <td>3.328903</td>\n",
       "      <td>3.365557</td>\n",
       "      <td>3.357815</td>\n",
       "      <td>3.421406</td>\n",
       "      <td>3.428380</td>\n",
       "      <td>0.992393</td>\n",
       "      <td>1.105412</td>\n",
       "      <td>1.483055</td>\n",
       "      <td>1.721859</td>\n",
       "      <td>1.699395</td>\n",
       "      <td>2.483675</td>\n",
       "      <td>4.277375</td>\n",
       "      <td>4.606743</td>\n",
       "      <td>4.764096</td>\n",
       "      <td>4.809254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2_P</td>\n",
       "      <td>44.4</td>\n",
       "      <td>0.191200</td>\n",
       "      <td>0.189831</td>\n",
       "      <td>0.187725</td>\n",
       "      <td>0.191629</td>\n",
       "      <td>0.196252</td>\n",
       "      <td>0.205140</td>\n",
       "      <td>0.209914</td>\n",
       "      <td>0.210256</td>\n",
       "      <td>0.210999</td>\n",
       "      <td>0.211794</td>\n",
       "      <td>0.214068</td>\n",
       "      <td>0.218355</td>\n",
       "      <td>0.221650</td>\n",
       "      <td>0.229133</td>\n",
       "      <td>0.235590</td>\n",
       "      <td>0.238543</td>\n",
       "      <td>0.242081</td>\n",
       "      <td>0.245498</td>\n",
       "      <td>0.248445</td>\n",
       "      <td>0.247065</td>\n",
       "      <td>0.249292</td>\n",
       "      <td>0.254473</td>\n",
       "      <td>0.263325</td>\n",
       "      <td>0.273994</td>\n",
       "      <td>0.279563</td>\n",
       "      <td>0.283124</td>\n",
       "      <td>0.288288</td>\n",
       "      <td>0.291484</td>\n",
       "      <td>0.296931</td>\n",
       "      <td>0.301143</td>\n",
       "      <td>0.306834</td>\n",
       "      <td>0.311672</td>\n",
       "      <td>0.315907</td>\n",
       "      <td>0.318279</td>\n",
       "      <td>0.319126</td>\n",
       "      <td>0.320443</td>\n",
       "      <td>0.324220</td>\n",
       "      <td>0.326946</td>\n",
       "      <td>0.331071</td>\n",
       "      <td>0.335602</td>\n",
       "      <td>0.340231</td>\n",
       "      <td>0.343282</td>\n",
       "      <td>0.345376</td>\n",
       "      <td>0.346862</td>\n",
       "      <td>0.350116</td>\n",
       "      <td>0.353510</td>\n",
       "      <td>0.355157</td>\n",
       "      <td>0.355238</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866659</td>\n",
       "      <td>0.868626</td>\n",
       "      <td>0.886784</td>\n",
       "      <td>0.897235</td>\n",
       "      <td>0.921281</td>\n",
       "      <td>0.942381</td>\n",
       "      <td>0.972106</td>\n",
       "      <td>0.993024</td>\n",
       "      <td>1.004513</td>\n",
       "      <td>1.016845</td>\n",
       "      <td>1.026514</td>\n",
       "      <td>1.040119</td>\n",
       "      <td>1.028430</td>\n",
       "      <td>1.024554</td>\n",
       "      <td>1.021568</td>\n",
       "      <td>1.045355</td>\n",
       "      <td>1.059899</td>\n",
       "      <td>1.062368</td>\n",
       "      <td>1.069155</td>\n",
       "      <td>1.073775</td>\n",
       "      <td>0.539424</td>\n",
       "      <td>0.545641</td>\n",
       "      <td>0.616830</td>\n",
       "      <td>0.695538</td>\n",
       "      <td>0.813433</td>\n",
       "      <td>0.904103</td>\n",
       "      <td>0.975803</td>\n",
       "      <td>1.019546</td>\n",
       "      <td>1.059941</td>\n",
       "      <td>1.061241</td>\n",
       "      <td>1.211937</td>\n",
       "      <td>1.250024</td>\n",
       "      <td>1.300998</td>\n",
       "      <td>1.373126</td>\n",
       "      <td>1.422106</td>\n",
       "      <td>1.455332</td>\n",
       "      <td>1.458446</td>\n",
       "      <td>1.450438</td>\n",
       "      <td>1.497318</td>\n",
       "      <td>1.510940</td>\n",
       "      <td>0.756372</td>\n",
       "      <td>0.862336</td>\n",
       "      <td>1.140623</td>\n",
       "      <td>1.386935</td>\n",
       "      <td>1.473936</td>\n",
       "      <td>1.671600</td>\n",
       "      <td>1.852783</td>\n",
       "      <td>2.015047</td>\n",
       "      <td>2.058019</td>\n",
       "      <td>2.104777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3_P</td>\n",
       "      <td>46.5</td>\n",
       "      <td>0.275959</td>\n",
       "      <td>0.276271</td>\n",
       "      <td>0.273568</td>\n",
       "      <td>0.278322</td>\n",
       "      <td>0.284468</td>\n",
       "      <td>0.296392</td>\n",
       "      <td>0.303449</td>\n",
       "      <td>0.305312</td>\n",
       "      <td>0.308380</td>\n",
       "      <td>0.311888</td>\n",
       "      <td>0.317236</td>\n",
       "      <td>0.324387</td>\n",
       "      <td>0.329835</td>\n",
       "      <td>0.341946</td>\n",
       "      <td>0.353035</td>\n",
       "      <td>0.358912</td>\n",
       "      <td>0.365899</td>\n",
       "      <td>0.372545</td>\n",
       "      <td>0.378533</td>\n",
       "      <td>0.377787</td>\n",
       "      <td>0.381994</td>\n",
       "      <td>0.391503</td>\n",
       "      <td>0.406517</td>\n",
       "      <td>0.423829</td>\n",
       "      <td>0.433760</td>\n",
       "      <td>0.440477</td>\n",
       "      <td>0.450120</td>\n",
       "      <td>0.456161</td>\n",
       "      <td>0.465329</td>\n",
       "      <td>0.471999</td>\n",
       "      <td>0.481131</td>\n",
       "      <td>0.488511</td>\n",
       "      <td>0.494952</td>\n",
       "      <td>0.499083</td>\n",
       "      <td>0.501164</td>\n",
       "      <td>0.503961</td>\n",
       "      <td>0.511077</td>\n",
       "      <td>0.516137</td>\n",
       "      <td>0.523359</td>\n",
       "      <td>0.530998</td>\n",
       "      <td>0.538473</td>\n",
       "      <td>0.542569</td>\n",
       "      <td>0.545731</td>\n",
       "      <td>0.547505</td>\n",
       "      <td>0.552389</td>\n",
       "      <td>0.557519</td>\n",
       "      <td>0.560445</td>\n",
       "      <td>0.560457</td>\n",
       "      <td>...</td>\n",
       "      <td>1.402975</td>\n",
       "      <td>1.377883</td>\n",
       "      <td>1.377740</td>\n",
       "      <td>1.365293</td>\n",
       "      <td>1.391699</td>\n",
       "      <td>1.412411</td>\n",
       "      <td>1.447749</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.498107</td>\n",
       "      <td>1.520896</td>\n",
       "      <td>1.538736</td>\n",
       "      <td>1.563145</td>\n",
       "      <td>1.564440</td>\n",
       "      <td>1.571650</td>\n",
       "      <td>1.567618</td>\n",
       "      <td>1.583163</td>\n",
       "      <td>1.595180</td>\n",
       "      <td>1.606258</td>\n",
       "      <td>1.621253</td>\n",
       "      <td>1.632857</td>\n",
       "      <td>0.778982</td>\n",
       "      <td>0.789126</td>\n",
       "      <td>0.909240</td>\n",
       "      <td>1.060155</td>\n",
       "      <td>1.267145</td>\n",
       "      <td>1.424162</td>\n",
       "      <td>1.539246</td>\n",
       "      <td>1.618779</td>\n",
       "      <td>1.676952</td>\n",
       "      <td>1.758562</td>\n",
       "      <td>1.970903</td>\n",
       "      <td>1.937199</td>\n",
       "      <td>1.963810</td>\n",
       "      <td>2.046768</td>\n",
       "      <td>2.122590</td>\n",
       "      <td>2.179101</td>\n",
       "      <td>2.217773</td>\n",
       "      <td>2.222106</td>\n",
       "      <td>2.254974</td>\n",
       "      <td>2.292981</td>\n",
       "      <td>1.090404</td>\n",
       "      <td>1.272584</td>\n",
       "      <td>1.779799</td>\n",
       "      <td>2.184225</td>\n",
       "      <td>2.368559</td>\n",
       "      <td>2.711463</td>\n",
       "      <td>2.792593</td>\n",
       "      <td>3.004298</td>\n",
       "      <td>3.125400</td>\n",
       "      <td>3.193575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4_P</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.182765</td>\n",
       "      <td>0.181889</td>\n",
       "      <td>0.179319</td>\n",
       "      <td>0.182695</td>\n",
       "      <td>0.186605</td>\n",
       "      <td>0.195498</td>\n",
       "      <td>0.200748</td>\n",
       "      <td>0.201549</td>\n",
       "      <td>0.202935</td>\n",
       "      <td>0.204745</td>\n",
       "      <td>0.207895</td>\n",
       "      <td>0.212507</td>\n",
       "      <td>0.216185</td>\n",
       "      <td>0.224991</td>\n",
       "      <td>0.232916</td>\n",
       "      <td>0.237620</td>\n",
       "      <td>0.243010</td>\n",
       "      <td>0.247981</td>\n",
       "      <td>0.252436</td>\n",
       "      <td>0.252210</td>\n",
       "      <td>0.255923</td>\n",
       "      <td>0.263203</td>\n",
       "      <td>0.274840</td>\n",
       "      <td>0.288038</td>\n",
       "      <td>0.295997</td>\n",
       "      <td>0.301619</td>\n",
       "      <td>0.309032</td>\n",
       "      <td>0.313894</td>\n",
       "      <td>0.321232</td>\n",
       "      <td>0.326894</td>\n",
       "      <td>0.334418</td>\n",
       "      <td>0.340672</td>\n",
       "      <td>0.346305</td>\n",
       "      <td>0.349907</td>\n",
       "      <td>0.352048</td>\n",
       "      <td>0.354171</td>\n",
       "      <td>0.359369</td>\n",
       "      <td>0.363499</td>\n",
       "      <td>0.369179</td>\n",
       "      <td>0.375728</td>\n",
       "      <td>0.382028</td>\n",
       "      <td>0.385979</td>\n",
       "      <td>0.389175</td>\n",
       "      <td>0.391292</td>\n",
       "      <td>0.395632</td>\n",
       "      <td>0.399925</td>\n",
       "      <td>0.402767</td>\n",
       "      <td>0.403544</td>\n",
       "      <td>...</td>\n",
       "      <td>1.047165</td>\n",
       "      <td>1.060899</td>\n",
       "      <td>1.092663</td>\n",
       "      <td>1.115769</td>\n",
       "      <td>1.152591</td>\n",
       "      <td>1.178323</td>\n",
       "      <td>1.213907</td>\n",
       "      <td>1.246973</td>\n",
       "      <td>1.264301</td>\n",
       "      <td>1.281568</td>\n",
       "      <td>1.296453</td>\n",
       "      <td>1.314961</td>\n",
       "      <td>1.311857</td>\n",
       "      <td>1.306006</td>\n",
       "      <td>1.286972</td>\n",
       "      <td>1.302869</td>\n",
       "      <td>1.310958</td>\n",
       "      <td>1.313102</td>\n",
       "      <td>1.318807</td>\n",
       "      <td>1.323288</td>\n",
       "      <td>0.515826</td>\n",
       "      <td>0.520212</td>\n",
       "      <td>0.597363</td>\n",
       "      <td>0.705795</td>\n",
       "      <td>0.870505</td>\n",
       "      <td>0.997854</td>\n",
       "      <td>1.098015</td>\n",
       "      <td>1.167249</td>\n",
       "      <td>1.225140</td>\n",
       "      <td>1.264590</td>\n",
       "      <td>1.466718</td>\n",
       "      <td>1.540797</td>\n",
       "      <td>1.626491</td>\n",
       "      <td>1.716734</td>\n",
       "      <td>1.790995</td>\n",
       "      <td>1.835513</td>\n",
       "      <td>1.858811</td>\n",
       "      <td>1.829229</td>\n",
       "      <td>1.852685</td>\n",
       "      <td>1.864653</td>\n",
       "      <td>0.721746</td>\n",
       "      <td>0.836694</td>\n",
       "      <td>1.217127</td>\n",
       "      <td>1.562283</td>\n",
       "      <td>1.710942</td>\n",
       "      <td>2.026881</td>\n",
       "      <td>2.311239</td>\n",
       "      <td>2.532482</td>\n",
       "      <td>2.615472</td>\n",
       "      <td>2.613160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2402 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Target  Value     feat1     feat2     feat3     feat4     feat5     feat6  \\\n",
       "0    0_P   45.1  0.203683  0.203992  0.202177  0.205636  0.210894  0.220505   \n",
       "1    1_P   44.8  0.250603  0.249785  0.246786  0.250632  0.255035  0.265170   \n",
       "2    2_P   44.4  0.191200  0.189831  0.187725  0.191629  0.196252  0.205140   \n",
       "3    3_P   46.5  0.275959  0.276271  0.273568  0.278322  0.284468  0.296392   \n",
       "4    4_P   52.0  0.182765  0.181889  0.179319  0.182695  0.186605  0.195498   \n",
       "\n",
       "      feat7     feat8     feat9    feat10    feat11    feat12    feat13  \\\n",
       "0  0.226073  0.227012  0.228581  0.230678  0.233982  0.238588  0.241646   \n",
       "1  0.270217  0.269648  0.269498  0.270211  0.272503  0.276662  0.279905   \n",
       "2  0.209914  0.210256  0.210999  0.211794  0.214068  0.218355  0.221650   \n",
       "3  0.303449  0.305312  0.308380  0.311888  0.317236  0.324387  0.329835   \n",
       "4  0.200748  0.201549  0.202935  0.204745  0.207895  0.212507  0.216185   \n",
       "\n",
       "     feat14    feat15    feat16    feat17    feat18    feat19    feat20  \\\n",
       "0  0.250014  0.258405  0.261899  0.266396  0.270737  0.274890  0.273968   \n",
       "1  0.288868  0.297204  0.301549  0.306996  0.313169  0.319590  0.320913   \n",
       "2  0.229133  0.235590  0.238543  0.242081  0.245498  0.248445  0.247065   \n",
       "3  0.341946  0.353035  0.358912  0.365899  0.372545  0.378533  0.377787   \n",
       "4  0.224991  0.232916  0.237620  0.243010  0.247981  0.252436  0.252210   \n",
       "\n",
       "     feat21    feat22    feat23    feat24    feat25    feat26    feat27  \\\n",
       "0  0.276430  0.283327  0.295028  0.308494  0.316209  0.320717  0.327333   \n",
       "1  0.326546  0.336216  0.350023  0.364886  0.372914  0.377352  0.383670   \n",
       "2  0.249292  0.254473  0.263325  0.273994  0.279563  0.283124  0.288288   \n",
       "3  0.381994  0.391503  0.406517  0.423829  0.433760  0.440477  0.450120   \n",
       "4  0.255923  0.263203  0.274840  0.288038  0.295997  0.301619  0.309032   \n",
       "\n",
       "     feat28    feat29    feat30    feat31    feat32    feat33    feat34  \\\n",
       "0  0.331691  0.337992  0.342740  0.349642  0.355668  0.360805  0.364883   \n",
       "1  0.387081  0.392847  0.396461  0.401595  0.405064  0.406857  0.406138   \n",
       "2  0.291484  0.296931  0.301143  0.306834  0.311672  0.315907  0.318279   \n",
       "3  0.456161  0.465329  0.471999  0.481131  0.488511  0.494952  0.499083   \n",
       "4  0.313894  0.321232  0.326894  0.334418  0.340672  0.346305  0.349907   \n",
       "\n",
       "     feat35    feat36    feat37    feat38    feat39    feat40    feat41  \\\n",
       "0  0.367570  0.370319  0.376717  0.381299  0.387101  0.393264  0.399336   \n",
       "1  0.403846  0.402338  0.404513  0.405366  0.408354  0.412235  0.416498   \n",
       "2  0.319126  0.320443  0.324220  0.326946  0.331071  0.335602  0.340231   \n",
       "3  0.501164  0.503961  0.511077  0.516137  0.523359  0.530998  0.538473   \n",
       "4  0.352048  0.354171  0.359369  0.363499  0.369179  0.375728  0.382028   \n",
       "\n",
       "     feat42    feat43    feat44    feat45    feat46    feat47    feat48  ...  \\\n",
       "0  0.402669  0.405688  0.407629  0.411576  0.416467  0.419428  0.419526  ...   \n",
       "1  0.418540  0.420223  0.421406  0.424962  0.428652  0.430207  0.429708  ...   \n",
       "2  0.343282  0.345376  0.346862  0.350116  0.353510  0.355157  0.355238  ...   \n",
       "3  0.542569  0.545731  0.547505  0.552389  0.557519  0.560445  0.560457  ...   \n",
       "4  0.385979  0.389175  0.391292  0.395632  0.399925  0.402767  0.403544  ...   \n",
       "\n",
       "   feat2351  feat2352  feat2353  feat2354  feat2355  feat2356  feat2357  \\\n",
       "0  1.080411  1.095705  1.129028  1.153727  1.191501  1.206518  1.240862   \n",
       "1  1.256190  1.502146  1.758616  2.011209  2.160844  2.194579  2.249652   \n",
       "2  0.866659  0.868626  0.886784  0.897235  0.921281  0.942381  0.972106   \n",
       "3  1.402975  1.377883  1.377740  1.365293  1.391699  1.412411  1.447749   \n",
       "4  1.047165  1.060899  1.092663  1.115769  1.152591  1.178323  1.213907   \n",
       "\n",
       "   feat2358  feat2359  feat2360  feat2361  feat2362  feat2363  feat2364  \\\n",
       "0  1.266370  1.281632  1.303487  1.320699  1.343543  1.349235  1.355927   \n",
       "1  2.285842  2.307155  2.331671  2.351600  2.378405  2.375424  2.375780   \n",
       "2  0.993024  1.004513  1.016845  1.026514  1.040119  1.028430  1.024554   \n",
       "3  1.480304  1.498107  1.520896  1.538736  1.563145  1.564440  1.571650   \n",
       "4  1.246973  1.264301  1.281568  1.296453  1.314961  1.311857  1.306006   \n",
       "\n",
       "   feat2365  feat2366  feat2367  feat2368  feat2369  feat2370  feat2371  \\\n",
       "0  1.355282  1.360762  1.357333  1.372652  1.387570  1.400359  0.574390   \n",
       "1  2.363581  2.404702  2.419109  2.419744  2.425575  2.429126  0.706865   \n",
       "2  1.021568  1.045355  1.059899  1.062368  1.069155  1.073775  0.539424   \n",
       "3  1.567618  1.583163  1.595180  1.606258  1.621253  1.632857  0.778982   \n",
       "4  1.286972  1.302869  1.310958  1.313102  1.318807  1.323288  0.515826   \n",
       "\n",
       "   feat2372  feat2373  feat2374  feat2375  feat2376  feat2377  feat2378  \\\n",
       "0  0.584549  0.670901  0.769619  0.920841  1.042641  1.146396  1.211927   \n",
       "1  0.714469  0.781618  0.895926  1.084361  1.143641  1.187382  1.231635   \n",
       "2  0.545641  0.616830  0.695538  0.813433  0.904103  0.975803  1.019546   \n",
       "3  0.789126  0.909240  1.060155  1.267145  1.424162  1.539246  1.618779   \n",
       "4  0.520212  0.597363  0.705795  0.870505  0.997854  1.098015  1.167249   \n",
       "\n",
       "   feat2379  feat2380  feat2381  feat2382  feat2383  feat2384  feat2385  \\\n",
       "0  1.273028  1.312649  1.513034  1.593172  1.678123  1.751818  1.816820   \n",
       "1  1.280067  1.232515  1.738577  2.511333  3.038777  3.175525  3.269756   \n",
       "2  1.059941  1.061241  1.211937  1.250024  1.300998  1.373126  1.422106   \n",
       "3  1.676952  1.758562  1.970903  1.937199  1.963810  2.046768  2.122590   \n",
       "4  1.225140  1.264590  1.466718  1.540797  1.626491  1.716734  1.790995   \n",
       "\n",
       "   feat2386  feat2387  feat2388  feat2389  feat2390  feat2391  feat2392  \\\n",
       "0  1.870275  1.911317  1.919118  1.924227  1.962802  0.805311  0.936756   \n",
       "1  3.328903  3.365557  3.357815  3.421406  3.428380  0.992393  1.105412   \n",
       "2  1.455332  1.458446  1.450438  1.497318  1.510940  0.756372  0.862336   \n",
       "3  2.179101  2.217773  2.222106  2.254974  2.292981  1.090404  1.272584   \n",
       "4  1.835513  1.858811  1.829229  1.852685  1.864653  0.721746  0.836694   \n",
       "\n",
       "   feat2393  feat2394  feat2395  feat2396  feat2397  feat2398  feat2399  \\\n",
       "0  1.290456  1.626534  1.777969  2.094892  2.382082  2.567942  2.697733   \n",
       "1  1.483055  1.721859  1.699395  2.483675  4.277375  4.606743  4.764096   \n",
       "2  1.140623  1.386935  1.473936  1.671600  1.852783  2.015047  2.058019   \n",
       "3  1.779799  2.184225  2.368559  2.711463  2.792593  3.004298  3.125400   \n",
       "4  1.217127  1.562283  1.710942  2.026881  2.311239  2.532482  2.615472   \n",
       "\n",
       "   feat2400  \n",
       "0  2.733829  \n",
       "1  4.809254  \n",
       "2  2.104777  \n",
       "3  3.193575  \n",
       "4  2.613160  \n",
       "\n",
       "[5 rows x 2402 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_columns = [\"P\", \"K\", \"Mg\", \"pH\"]\n",
    "train.drop(columns=dummy_columns+['sample_index'],inplace=True)\n",
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "id": "r0unIVyIR0Zq",
    "outputId": "18f2716c-4810-4e13-f678-24300c267783",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4616, 2402)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat1</th>\n",
       "      <th>feat2</th>\n",
       "      <th>feat3</th>\n",
       "      <th>feat4</th>\n",
       "      <th>feat5</th>\n",
       "      <th>feat6</th>\n",
       "      <th>feat7</th>\n",
       "      <th>feat8</th>\n",
       "      <th>feat9</th>\n",
       "      <th>feat10</th>\n",
       "      <th>feat11</th>\n",
       "      <th>feat12</th>\n",
       "      <th>feat13</th>\n",
       "      <th>feat14</th>\n",
       "      <th>feat15</th>\n",
       "      <th>feat16</th>\n",
       "      <th>feat17</th>\n",
       "      <th>feat18</th>\n",
       "      <th>feat19</th>\n",
       "      <th>feat20</th>\n",
       "      <th>feat21</th>\n",
       "      <th>feat22</th>\n",
       "      <th>feat23</th>\n",
       "      <th>feat24</th>\n",
       "      <th>feat25</th>\n",
       "      <th>feat26</th>\n",
       "      <th>feat27</th>\n",
       "      <th>feat28</th>\n",
       "      <th>feat29</th>\n",
       "      <th>feat30</th>\n",
       "      <th>feat31</th>\n",
       "      <th>feat32</th>\n",
       "      <th>feat33</th>\n",
       "      <th>feat34</th>\n",
       "      <th>feat35</th>\n",
       "      <th>feat36</th>\n",
       "      <th>feat37</th>\n",
       "      <th>feat38</th>\n",
       "      <th>feat39</th>\n",
       "      <th>feat40</th>\n",
       "      <th>feat41</th>\n",
       "      <th>feat42</th>\n",
       "      <th>feat43</th>\n",
       "      <th>feat44</th>\n",
       "      <th>feat45</th>\n",
       "      <th>feat46</th>\n",
       "      <th>feat47</th>\n",
       "      <th>feat48</th>\n",
       "      <th>feat49</th>\n",
       "      <th>feat50</th>\n",
       "      <th>...</th>\n",
       "      <th>feat2353</th>\n",
       "      <th>feat2354</th>\n",
       "      <th>feat2355</th>\n",
       "      <th>feat2356</th>\n",
       "      <th>feat2357</th>\n",
       "      <th>feat2358</th>\n",
       "      <th>feat2359</th>\n",
       "      <th>feat2360</th>\n",
       "      <th>feat2361</th>\n",
       "      <th>feat2362</th>\n",
       "      <th>feat2363</th>\n",
       "      <th>feat2364</th>\n",
       "      <th>feat2365</th>\n",
       "      <th>feat2366</th>\n",
       "      <th>feat2367</th>\n",
       "      <th>feat2368</th>\n",
       "      <th>feat2369</th>\n",
       "      <th>feat2370</th>\n",
       "      <th>feat2371</th>\n",
       "      <th>feat2372</th>\n",
       "      <th>feat2373</th>\n",
       "      <th>feat2374</th>\n",
       "      <th>feat2375</th>\n",
       "      <th>feat2376</th>\n",
       "      <th>feat2377</th>\n",
       "      <th>feat2378</th>\n",
       "      <th>feat2379</th>\n",
       "      <th>feat2380</th>\n",
       "      <th>feat2381</th>\n",
       "      <th>feat2382</th>\n",
       "      <th>feat2383</th>\n",
       "      <th>feat2384</th>\n",
       "      <th>feat2385</th>\n",
       "      <th>feat2386</th>\n",
       "      <th>feat2387</th>\n",
       "      <th>feat2388</th>\n",
       "      <th>feat2389</th>\n",
       "      <th>feat2390</th>\n",
       "      <th>feat2391</th>\n",
       "      <th>feat2392</th>\n",
       "      <th>feat2393</th>\n",
       "      <th>feat2394</th>\n",
       "      <th>feat2395</th>\n",
       "      <th>feat2396</th>\n",
       "      <th>feat2397</th>\n",
       "      <th>feat2398</th>\n",
       "      <th>feat2399</th>\n",
       "      <th>feat2400</th>\n",
       "      <th>Target</th>\n",
       "      <th>mean_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.173565</td>\n",
       "      <td>0.171952</td>\n",
       "      <td>0.168504</td>\n",
       "      <td>0.170999</td>\n",
       "      <td>0.173946</td>\n",
       "      <td>0.181525</td>\n",
       "      <td>0.185026</td>\n",
       "      <td>0.184056</td>\n",
       "      <td>0.183710</td>\n",
       "      <td>0.183698</td>\n",
       "      <td>0.184985</td>\n",
       "      <td>0.187467</td>\n",
       "      <td>0.189658</td>\n",
       "      <td>0.196452</td>\n",
       "      <td>0.202846</td>\n",
       "      <td>0.207412</td>\n",
       "      <td>0.213938</td>\n",
       "      <td>0.222476</td>\n",
       "      <td>0.232073</td>\n",
       "      <td>0.238128</td>\n",
       "      <td>0.247958</td>\n",
       "      <td>0.260854</td>\n",
       "      <td>0.276417</td>\n",
       "      <td>0.291528</td>\n",
       "      <td>0.300351</td>\n",
       "      <td>0.305525</td>\n",
       "      <td>0.311848</td>\n",
       "      <td>0.315623</td>\n",
       "      <td>0.320844</td>\n",
       "      <td>0.323720</td>\n",
       "      <td>0.327251</td>\n",
       "      <td>0.327792</td>\n",
       "      <td>0.325899</td>\n",
       "      <td>0.320993</td>\n",
       "      <td>0.314128</td>\n",
       "      <td>0.308204</td>\n",
       "      <td>0.305358</td>\n",
       "      <td>0.301816</td>\n",
       "      <td>0.300822</td>\n",
       "      <td>0.301477</td>\n",
       "      <td>0.303310</td>\n",
       "      <td>0.303769</td>\n",
       "      <td>0.304400</td>\n",
       "      <td>0.304489</td>\n",
       "      <td>0.306138</td>\n",
       "      <td>0.307097</td>\n",
       "      <td>0.305769</td>\n",
       "      <td>0.302423</td>\n",
       "      <td>0.300566</td>\n",
       "      <td>0.299315</td>\n",
       "      <td>...</td>\n",
       "      <td>1.688006</td>\n",
       "      <td>2.042916</td>\n",
       "      <td>2.297939</td>\n",
       "      <td>2.377649</td>\n",
       "      <td>2.466454</td>\n",
       "      <td>2.507972</td>\n",
       "      <td>2.530190</td>\n",
       "      <td>2.559515</td>\n",
       "      <td>2.581921</td>\n",
       "      <td>2.615295</td>\n",
       "      <td>2.617831</td>\n",
       "      <td>2.625087</td>\n",
       "      <td>2.613590</td>\n",
       "      <td>2.660765</td>\n",
       "      <td>2.680289</td>\n",
       "      <td>2.683594</td>\n",
       "      <td>2.687329</td>\n",
       "      <td>2.684616</td>\n",
       "      <td>0.489267</td>\n",
       "      <td>0.491960</td>\n",
       "      <td>0.528080</td>\n",
       "      <td>0.650628</td>\n",
       "      <td>0.887739</td>\n",
       "      <td>0.887057</td>\n",
       "      <td>0.858053</td>\n",
       "      <td>0.846928</td>\n",
       "      <td>0.843302</td>\n",
       "      <td>0.756928</td>\n",
       "      <td>1.354611</td>\n",
       "      <td>2.418203</td>\n",
       "      <td>3.218510</td>\n",
       "      <td>3.481309</td>\n",
       "      <td>3.586095</td>\n",
       "      <td>3.656175</td>\n",
       "      <td>3.709706</td>\n",
       "      <td>3.713505</td>\n",
       "      <td>3.791356</td>\n",
       "      <td>3.797913</td>\n",
       "      <td>0.688768</td>\n",
       "      <td>0.754274</td>\n",
       "      <td>1.174493</td>\n",
       "      <td>1.269823</td>\n",
       "      <td>1.064050</td>\n",
       "      <td>1.963784</td>\n",
       "      <td>4.521961</td>\n",
       "      <td>5.065660</td>\n",
       "      <td>5.246762</td>\n",
       "      <td>5.330273</td>\n",
       "      <td>0_P</td>\n",
       "      <td>70.302656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.202796</td>\n",
       "      <td>0.201845</td>\n",
       "      <td>0.199020</td>\n",
       "      <td>0.202500</td>\n",
       "      <td>0.206132</td>\n",
       "      <td>0.214863</td>\n",
       "      <td>0.219335</td>\n",
       "      <td>0.218668</td>\n",
       "      <td>0.218889</td>\n",
       "      <td>0.219440</td>\n",
       "      <td>0.221122</td>\n",
       "      <td>0.224898</td>\n",
       "      <td>0.227561</td>\n",
       "      <td>0.236100</td>\n",
       "      <td>0.243862</td>\n",
       "      <td>0.248254</td>\n",
       "      <td>0.253956</td>\n",
       "      <td>0.260094</td>\n",
       "      <td>0.266156</td>\n",
       "      <td>0.267235</td>\n",
       "      <td>0.272653</td>\n",
       "      <td>0.281744</td>\n",
       "      <td>0.294286</td>\n",
       "      <td>0.307994</td>\n",
       "      <td>0.314845</td>\n",
       "      <td>0.318900</td>\n",
       "      <td>0.324794</td>\n",
       "      <td>0.327503</td>\n",
       "      <td>0.332724</td>\n",
       "      <td>0.336211</td>\n",
       "      <td>0.341693</td>\n",
       "      <td>0.345040</td>\n",
       "      <td>0.347407</td>\n",
       "      <td>0.347685</td>\n",
       "      <td>0.346067</td>\n",
       "      <td>0.344727</td>\n",
       "      <td>0.346363</td>\n",
       "      <td>0.347024</td>\n",
       "      <td>0.349843</td>\n",
       "      <td>0.353515</td>\n",
       "      <td>0.357605</td>\n",
       "      <td>0.359821</td>\n",
       "      <td>0.362217</td>\n",
       "      <td>0.363633</td>\n",
       "      <td>0.367676</td>\n",
       "      <td>0.371853</td>\n",
       "      <td>0.374272</td>\n",
       "      <td>0.374649</td>\n",
       "      <td>0.376807</td>\n",
       "      <td>0.379905</td>\n",
       "      <td>...</td>\n",
       "      <td>1.798136</td>\n",
       "      <td>2.096276</td>\n",
       "      <td>2.280492</td>\n",
       "      <td>2.336384</td>\n",
       "      <td>2.416731</td>\n",
       "      <td>2.466844</td>\n",
       "      <td>2.495732</td>\n",
       "      <td>2.528185</td>\n",
       "      <td>2.554744</td>\n",
       "      <td>2.590548</td>\n",
       "      <td>2.593327</td>\n",
       "      <td>2.601961</td>\n",
       "      <td>2.589237</td>\n",
       "      <td>2.643366</td>\n",
       "      <td>2.655447</td>\n",
       "      <td>2.637913</td>\n",
       "      <td>2.626337</td>\n",
       "      <td>2.615737</td>\n",
       "      <td>0.571762</td>\n",
       "      <td>0.577881</td>\n",
       "      <td>0.635002</td>\n",
       "      <td>0.744523</td>\n",
       "      <td>0.917573</td>\n",
       "      <td>0.976870</td>\n",
       "      <td>1.023253</td>\n",
       "      <td>1.082452</td>\n",
       "      <td>1.146555</td>\n",
       "      <td>1.090484</td>\n",
       "      <td>1.659797</td>\n",
       "      <td>2.571110</td>\n",
       "      <td>3.205332</td>\n",
       "      <td>3.410100</td>\n",
       "      <td>3.538129</td>\n",
       "      <td>3.616678</td>\n",
       "      <td>3.675339</td>\n",
       "      <td>3.682659</td>\n",
       "      <td>3.754580</td>\n",
       "      <td>3.710804</td>\n",
       "      <td>0.800777</td>\n",
       "      <td>0.901758</td>\n",
       "      <td>1.248479</td>\n",
       "      <td>1.495482</td>\n",
       "      <td>1.494265</td>\n",
       "      <td>2.381440</td>\n",
       "      <td>4.517328</td>\n",
       "      <td>4.982208</td>\n",
       "      <td>5.206562</td>\n",
       "      <td>5.262480</td>\n",
       "      <td>1_P</td>\n",
       "      <td>70.302656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.164810</td>\n",
       "      <td>0.163459</td>\n",
       "      <td>0.161074</td>\n",
       "      <td>0.163151</td>\n",
       "      <td>0.166365</td>\n",
       "      <td>0.173361</td>\n",
       "      <td>0.176534</td>\n",
       "      <td>0.175363</td>\n",
       "      <td>0.174817</td>\n",
       "      <td>0.174266</td>\n",
       "      <td>0.175142</td>\n",
       "      <td>0.177567</td>\n",
       "      <td>0.179800</td>\n",
       "      <td>0.186686</td>\n",
       "      <td>0.194100</td>\n",
       "      <td>0.199420</td>\n",
       "      <td>0.207205</td>\n",
       "      <td>0.216406</td>\n",
       "      <td>0.226819</td>\n",
       "      <td>0.234233</td>\n",
       "      <td>0.245464</td>\n",
       "      <td>0.259351</td>\n",
       "      <td>0.275583</td>\n",
       "      <td>0.291327</td>\n",
       "      <td>0.300969</td>\n",
       "      <td>0.306700</td>\n",
       "      <td>0.313685</td>\n",
       "      <td>0.318030</td>\n",
       "      <td>0.323988</td>\n",
       "      <td>0.326964</td>\n",
       "      <td>0.330085</td>\n",
       "      <td>0.330050</td>\n",
       "      <td>0.327086</td>\n",
       "      <td>0.321105</td>\n",
       "      <td>0.313493</td>\n",
       "      <td>0.306747</td>\n",
       "      <td>0.303655</td>\n",
       "      <td>0.299919</td>\n",
       "      <td>0.298985</td>\n",
       "      <td>0.299455</td>\n",
       "      <td>0.300853</td>\n",
       "      <td>0.300812</td>\n",
       "      <td>0.300627</td>\n",
       "      <td>0.300261</td>\n",
       "      <td>0.301276</td>\n",
       "      <td>0.301984</td>\n",
       "      <td>0.300389</td>\n",
       "      <td>0.296479</td>\n",
       "      <td>0.293648</td>\n",
       "      <td>0.291786</td>\n",
       "      <td>...</td>\n",
       "      <td>1.989744</td>\n",
       "      <td>2.470337</td>\n",
       "      <td>2.807123</td>\n",
       "      <td>2.898035</td>\n",
       "      <td>3.001448</td>\n",
       "      <td>3.048183</td>\n",
       "      <td>3.074964</td>\n",
       "      <td>3.106142</td>\n",
       "      <td>3.131777</td>\n",
       "      <td>3.165942</td>\n",
       "      <td>3.161512</td>\n",
       "      <td>3.161910</td>\n",
       "      <td>3.150329</td>\n",
       "      <td>3.199207</td>\n",
       "      <td>3.225819</td>\n",
       "      <td>3.235964</td>\n",
       "      <td>3.238828</td>\n",
       "      <td>3.231759</td>\n",
       "      <td>0.464090</td>\n",
       "      <td>0.469189</td>\n",
       "      <td>0.501251</td>\n",
       "      <td>0.635263</td>\n",
       "      <td>0.894274</td>\n",
       "      <td>0.885812</td>\n",
       "      <td>0.848520</td>\n",
       "      <td>0.821781</td>\n",
       "      <td>0.808230</td>\n",
       "      <td>0.666904</td>\n",
       "      <td>1.421769</td>\n",
       "      <td>2.859085</td>\n",
       "      <td>3.925901</td>\n",
       "      <td>4.236832</td>\n",
       "      <td>4.356789</td>\n",
       "      <td>4.433868</td>\n",
       "      <td>4.480423</td>\n",
       "      <td>4.470493</td>\n",
       "      <td>4.564563</td>\n",
       "      <td>4.577605</td>\n",
       "      <td>0.653497</td>\n",
       "      <td>0.721019</td>\n",
       "      <td>1.169714</td>\n",
       "      <td>1.269090</td>\n",
       "      <td>0.967839</td>\n",
       "      <td>2.094396</td>\n",
       "      <td>5.501404</td>\n",
       "      <td>6.153006</td>\n",
       "      <td>6.340413</td>\n",
       "      <td>6.413906</td>\n",
       "      <td>2_P</td>\n",
       "      <td>70.302656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 2402 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      feat1     feat2     feat3     feat4     feat5     feat6     feat7  \\\n",
       "0  0.173565  0.171952  0.168504  0.170999  0.173946  0.181525  0.185026   \n",
       "1  0.202796  0.201845  0.199020  0.202500  0.206132  0.214863  0.219335   \n",
       "2  0.164810  0.163459  0.161074  0.163151  0.166365  0.173361  0.176534   \n",
       "\n",
       "      feat8     feat9    feat10    feat11    feat12    feat13    feat14  \\\n",
       "0  0.184056  0.183710  0.183698  0.184985  0.187467  0.189658  0.196452   \n",
       "1  0.218668  0.218889  0.219440  0.221122  0.224898  0.227561  0.236100   \n",
       "2  0.175363  0.174817  0.174266  0.175142  0.177567  0.179800  0.186686   \n",
       "\n",
       "     feat15    feat16    feat17    feat18    feat19    feat20    feat21  \\\n",
       "0  0.202846  0.207412  0.213938  0.222476  0.232073  0.238128  0.247958   \n",
       "1  0.243862  0.248254  0.253956  0.260094  0.266156  0.267235  0.272653   \n",
       "2  0.194100  0.199420  0.207205  0.216406  0.226819  0.234233  0.245464   \n",
       "\n",
       "     feat22    feat23    feat24    feat25    feat26    feat27    feat28  \\\n",
       "0  0.260854  0.276417  0.291528  0.300351  0.305525  0.311848  0.315623   \n",
       "1  0.281744  0.294286  0.307994  0.314845  0.318900  0.324794  0.327503   \n",
       "2  0.259351  0.275583  0.291327  0.300969  0.306700  0.313685  0.318030   \n",
       "\n",
       "     feat29    feat30    feat31    feat32    feat33    feat34    feat35  \\\n",
       "0  0.320844  0.323720  0.327251  0.327792  0.325899  0.320993  0.314128   \n",
       "1  0.332724  0.336211  0.341693  0.345040  0.347407  0.347685  0.346067   \n",
       "2  0.323988  0.326964  0.330085  0.330050  0.327086  0.321105  0.313493   \n",
       "\n",
       "     feat36    feat37    feat38    feat39    feat40    feat41    feat42  \\\n",
       "0  0.308204  0.305358  0.301816  0.300822  0.301477  0.303310  0.303769   \n",
       "1  0.344727  0.346363  0.347024  0.349843  0.353515  0.357605  0.359821   \n",
       "2  0.306747  0.303655  0.299919  0.298985  0.299455  0.300853  0.300812   \n",
       "\n",
       "     feat43    feat44    feat45    feat46    feat47    feat48    feat49  \\\n",
       "0  0.304400  0.304489  0.306138  0.307097  0.305769  0.302423  0.300566   \n",
       "1  0.362217  0.363633  0.367676  0.371853  0.374272  0.374649  0.376807   \n",
       "2  0.300627  0.300261  0.301276  0.301984  0.300389  0.296479  0.293648   \n",
       "\n",
       "     feat50  ...  feat2353  feat2354  feat2355  feat2356  feat2357  feat2358  \\\n",
       "0  0.299315  ...  1.688006  2.042916  2.297939  2.377649  2.466454  2.507972   \n",
       "1  0.379905  ...  1.798136  2.096276  2.280492  2.336384  2.416731  2.466844   \n",
       "2  0.291786  ...  1.989744  2.470337  2.807123  2.898035  3.001448  3.048183   \n",
       "\n",
       "   feat2359  feat2360  feat2361  feat2362  feat2363  feat2364  feat2365  \\\n",
       "0  2.530190  2.559515  2.581921  2.615295  2.617831  2.625087  2.613590   \n",
       "1  2.495732  2.528185  2.554744  2.590548  2.593327  2.601961  2.589237   \n",
       "2  3.074964  3.106142  3.131777  3.165942  3.161512  3.161910  3.150329   \n",
       "\n",
       "   feat2366  feat2367  feat2368  feat2369  feat2370  feat2371  feat2372  \\\n",
       "0  2.660765  2.680289  2.683594  2.687329  2.684616  0.489267  0.491960   \n",
       "1  2.643366  2.655447  2.637913  2.626337  2.615737  0.571762  0.577881   \n",
       "2  3.199207  3.225819  3.235964  3.238828  3.231759  0.464090  0.469189   \n",
       "\n",
       "   feat2373  feat2374  feat2375  feat2376  feat2377  feat2378  feat2379  \\\n",
       "0  0.528080  0.650628  0.887739  0.887057  0.858053  0.846928  0.843302   \n",
       "1  0.635002  0.744523  0.917573  0.976870  1.023253  1.082452  1.146555   \n",
       "2  0.501251  0.635263  0.894274  0.885812  0.848520  0.821781  0.808230   \n",
       "\n",
       "   feat2380  feat2381  feat2382  feat2383  feat2384  feat2385  feat2386  \\\n",
       "0  0.756928  1.354611  2.418203  3.218510  3.481309  3.586095  3.656175   \n",
       "1  1.090484  1.659797  2.571110  3.205332  3.410100  3.538129  3.616678   \n",
       "2  0.666904  1.421769  2.859085  3.925901  4.236832  4.356789  4.433868   \n",
       "\n",
       "   feat2387  feat2388  feat2389  feat2390  feat2391  feat2392  feat2393  \\\n",
       "0  3.709706  3.713505  3.791356  3.797913  0.688768  0.754274  1.174493   \n",
       "1  3.675339  3.682659  3.754580  3.710804  0.800777  0.901758  1.248479   \n",
       "2  4.480423  4.470493  4.564563  4.577605  0.653497  0.721019  1.169714   \n",
       "\n",
       "   feat2394  feat2395  feat2396  feat2397  feat2398  feat2399  feat2400  \\\n",
       "0  1.269823  1.064050  1.963784  4.521961  5.065660  5.246762  5.330273   \n",
       "1  1.495482  1.494265  2.381440  4.517328  4.982208  5.206562  5.262480   \n",
       "2  1.269090  0.967839  2.094396  5.501404  6.153006  6.340413  6.413906   \n",
       "\n",
       "   Target  mean_norm  \n",
       "0     0_P  70.302656  \n",
       "1     1_P  70.302656  \n",
       "2     2_P  70.302656  \n",
       "\n",
       "[3 rows x 2402 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test.shape)\n",
    "test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "M1cox4McUNC7"
   },
   "outputs": [],
   "source": [
    "#print(sample_submission.shape)\n",
    "#sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "U9emy9_O_wzs",
    "outputId": "c860beeb-3381-4cce-cfa8-71e2f50219df"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat1</th>\n",
       "      <th>feat2</th>\n",
       "      <th>feat3</th>\n",
       "      <th>feat4</th>\n",
       "      <th>feat5</th>\n",
       "      <th>feat6</th>\n",
       "      <th>feat7</th>\n",
       "      <th>feat8</th>\n",
       "      <th>feat9</th>\n",
       "      <th>feat10</th>\n",
       "      <th>feat11</th>\n",
       "      <th>feat12</th>\n",
       "      <th>feat13</th>\n",
       "      <th>feat14</th>\n",
       "      <th>feat15</th>\n",
       "      <th>feat16</th>\n",
       "      <th>feat17</th>\n",
       "      <th>feat18</th>\n",
       "      <th>feat19</th>\n",
       "      <th>feat20</th>\n",
       "      <th>feat21</th>\n",
       "      <th>feat22</th>\n",
       "      <th>feat23</th>\n",
       "      <th>feat24</th>\n",
       "      <th>feat25</th>\n",
       "      <th>feat26</th>\n",
       "      <th>feat27</th>\n",
       "      <th>feat28</th>\n",
       "      <th>feat29</th>\n",
       "      <th>feat30</th>\n",
       "      <th>feat31</th>\n",
       "      <th>feat32</th>\n",
       "      <th>feat33</th>\n",
       "      <th>feat34</th>\n",
       "      <th>feat35</th>\n",
       "      <th>feat36</th>\n",
       "      <th>feat37</th>\n",
       "      <th>feat38</th>\n",
       "      <th>feat39</th>\n",
       "      <th>feat40</th>\n",
       "      <th>feat41</th>\n",
       "      <th>feat42</th>\n",
       "      <th>feat43</th>\n",
       "      <th>feat44</th>\n",
       "      <th>feat45</th>\n",
       "      <th>feat46</th>\n",
       "      <th>feat47</th>\n",
       "      <th>feat48</th>\n",
       "      <th>feat49</th>\n",
       "      <th>feat50</th>\n",
       "      <th>...</th>\n",
       "      <th>feat2354</th>\n",
       "      <th>feat2355</th>\n",
       "      <th>feat2356</th>\n",
       "      <th>feat2357</th>\n",
       "      <th>feat2358</th>\n",
       "      <th>feat2359</th>\n",
       "      <th>feat2360</th>\n",
       "      <th>feat2361</th>\n",
       "      <th>feat2362</th>\n",
       "      <th>feat2363</th>\n",
       "      <th>feat2364</th>\n",
       "      <th>feat2365</th>\n",
       "      <th>feat2366</th>\n",
       "      <th>feat2367</th>\n",
       "      <th>feat2368</th>\n",
       "      <th>feat2369</th>\n",
       "      <th>feat2370</th>\n",
       "      <th>feat2371</th>\n",
       "      <th>feat2372</th>\n",
       "      <th>feat2373</th>\n",
       "      <th>feat2374</th>\n",
       "      <th>feat2375</th>\n",
       "      <th>feat2376</th>\n",
       "      <th>feat2377</th>\n",
       "      <th>feat2378</th>\n",
       "      <th>feat2379</th>\n",
       "      <th>feat2380</th>\n",
       "      <th>feat2381</th>\n",
       "      <th>feat2382</th>\n",
       "      <th>feat2383</th>\n",
       "      <th>feat2384</th>\n",
       "      <th>feat2385</th>\n",
       "      <th>feat2386</th>\n",
       "      <th>feat2387</th>\n",
       "      <th>feat2388</th>\n",
       "      <th>feat2389</th>\n",
       "      <th>feat2390</th>\n",
       "      <th>feat2391</th>\n",
       "      <th>feat2392</th>\n",
       "      <th>feat2393</th>\n",
       "      <th>feat2394</th>\n",
       "      <th>feat2395</th>\n",
       "      <th>feat2396</th>\n",
       "      <th>feat2397</th>\n",
       "      <th>feat2398</th>\n",
       "      <th>feat2399</th>\n",
       "      <th>feat2400</th>\n",
       "      <th>Target</th>\n",
       "      <th>mean_norm</th>\n",
       "      <th>Param</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.173565</td>\n",
       "      <td>0.171952</td>\n",
       "      <td>0.168504</td>\n",
       "      <td>0.170999</td>\n",
       "      <td>0.173946</td>\n",
       "      <td>0.181525</td>\n",
       "      <td>0.185026</td>\n",
       "      <td>0.184056</td>\n",
       "      <td>0.183710</td>\n",
       "      <td>0.183698</td>\n",
       "      <td>0.184985</td>\n",
       "      <td>0.187467</td>\n",
       "      <td>0.189658</td>\n",
       "      <td>0.196452</td>\n",
       "      <td>0.202846</td>\n",
       "      <td>0.207412</td>\n",
       "      <td>0.213938</td>\n",
       "      <td>0.222476</td>\n",
       "      <td>0.232073</td>\n",
       "      <td>0.238128</td>\n",
       "      <td>0.247958</td>\n",
       "      <td>0.260854</td>\n",
       "      <td>0.276417</td>\n",
       "      <td>0.291528</td>\n",
       "      <td>0.300351</td>\n",
       "      <td>0.305525</td>\n",
       "      <td>0.311848</td>\n",
       "      <td>0.315623</td>\n",
       "      <td>0.320844</td>\n",
       "      <td>0.323720</td>\n",
       "      <td>0.327251</td>\n",
       "      <td>0.327792</td>\n",
       "      <td>0.325899</td>\n",
       "      <td>0.320993</td>\n",
       "      <td>0.314128</td>\n",
       "      <td>0.308204</td>\n",
       "      <td>0.305358</td>\n",
       "      <td>0.301816</td>\n",
       "      <td>0.300822</td>\n",
       "      <td>0.301477</td>\n",
       "      <td>0.303310</td>\n",
       "      <td>0.303769</td>\n",
       "      <td>0.304400</td>\n",
       "      <td>0.304489</td>\n",
       "      <td>0.306138</td>\n",
       "      <td>0.307097</td>\n",
       "      <td>0.305769</td>\n",
       "      <td>0.302423</td>\n",
       "      <td>0.300566</td>\n",
       "      <td>0.299315</td>\n",
       "      <td>...</td>\n",
       "      <td>2.042916</td>\n",
       "      <td>2.297939</td>\n",
       "      <td>2.377649</td>\n",
       "      <td>2.466454</td>\n",
       "      <td>2.507972</td>\n",
       "      <td>2.530190</td>\n",
       "      <td>2.559515</td>\n",
       "      <td>2.581921</td>\n",
       "      <td>2.615295</td>\n",
       "      <td>2.617831</td>\n",
       "      <td>2.625087</td>\n",
       "      <td>2.613590</td>\n",
       "      <td>2.660765</td>\n",
       "      <td>2.680289</td>\n",
       "      <td>2.683594</td>\n",
       "      <td>2.687329</td>\n",
       "      <td>2.684616</td>\n",
       "      <td>0.489267</td>\n",
       "      <td>0.491960</td>\n",
       "      <td>0.528080</td>\n",
       "      <td>0.650628</td>\n",
       "      <td>0.887739</td>\n",
       "      <td>0.887057</td>\n",
       "      <td>0.858053</td>\n",
       "      <td>0.846928</td>\n",
       "      <td>0.843302</td>\n",
       "      <td>0.756928</td>\n",
       "      <td>1.354611</td>\n",
       "      <td>2.418203</td>\n",
       "      <td>3.218510</td>\n",
       "      <td>3.481309</td>\n",
       "      <td>3.586095</td>\n",
       "      <td>3.656175</td>\n",
       "      <td>3.709706</td>\n",
       "      <td>3.713505</td>\n",
       "      <td>3.791356</td>\n",
       "      <td>3.797913</td>\n",
       "      <td>0.688768</td>\n",
       "      <td>0.754274</td>\n",
       "      <td>1.174493</td>\n",
       "      <td>1.269823</td>\n",
       "      <td>1.064050</td>\n",
       "      <td>1.963784</td>\n",
       "      <td>4.521961</td>\n",
       "      <td>5.065660</td>\n",
       "      <td>5.246762</td>\n",
       "      <td>5.330273</td>\n",
       "      <td>0_P</td>\n",
       "      <td>70.302656</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.202796</td>\n",
       "      <td>0.201845</td>\n",
       "      <td>0.199020</td>\n",
       "      <td>0.202500</td>\n",
       "      <td>0.206132</td>\n",
       "      <td>0.214863</td>\n",
       "      <td>0.219335</td>\n",
       "      <td>0.218668</td>\n",
       "      <td>0.218889</td>\n",
       "      <td>0.219440</td>\n",
       "      <td>0.221122</td>\n",
       "      <td>0.224898</td>\n",
       "      <td>0.227561</td>\n",
       "      <td>0.236100</td>\n",
       "      <td>0.243862</td>\n",
       "      <td>0.248254</td>\n",
       "      <td>0.253956</td>\n",
       "      <td>0.260094</td>\n",
       "      <td>0.266156</td>\n",
       "      <td>0.267235</td>\n",
       "      <td>0.272653</td>\n",
       "      <td>0.281744</td>\n",
       "      <td>0.294286</td>\n",
       "      <td>0.307994</td>\n",
       "      <td>0.314845</td>\n",
       "      <td>0.318900</td>\n",
       "      <td>0.324794</td>\n",
       "      <td>0.327503</td>\n",
       "      <td>0.332724</td>\n",
       "      <td>0.336211</td>\n",
       "      <td>0.341693</td>\n",
       "      <td>0.345040</td>\n",
       "      <td>0.347407</td>\n",
       "      <td>0.347685</td>\n",
       "      <td>0.346067</td>\n",
       "      <td>0.344727</td>\n",
       "      <td>0.346363</td>\n",
       "      <td>0.347024</td>\n",
       "      <td>0.349843</td>\n",
       "      <td>0.353515</td>\n",
       "      <td>0.357605</td>\n",
       "      <td>0.359821</td>\n",
       "      <td>0.362217</td>\n",
       "      <td>0.363633</td>\n",
       "      <td>0.367676</td>\n",
       "      <td>0.371853</td>\n",
       "      <td>0.374272</td>\n",
       "      <td>0.374649</td>\n",
       "      <td>0.376807</td>\n",
       "      <td>0.379905</td>\n",
       "      <td>...</td>\n",
       "      <td>2.096276</td>\n",
       "      <td>2.280492</td>\n",
       "      <td>2.336384</td>\n",
       "      <td>2.416731</td>\n",
       "      <td>2.466844</td>\n",
       "      <td>2.495732</td>\n",
       "      <td>2.528185</td>\n",
       "      <td>2.554744</td>\n",
       "      <td>2.590548</td>\n",
       "      <td>2.593327</td>\n",
       "      <td>2.601961</td>\n",
       "      <td>2.589237</td>\n",
       "      <td>2.643366</td>\n",
       "      <td>2.655447</td>\n",
       "      <td>2.637913</td>\n",
       "      <td>2.626337</td>\n",
       "      <td>2.615737</td>\n",
       "      <td>0.571762</td>\n",
       "      <td>0.577881</td>\n",
       "      <td>0.635002</td>\n",
       "      <td>0.744523</td>\n",
       "      <td>0.917573</td>\n",
       "      <td>0.976870</td>\n",
       "      <td>1.023253</td>\n",
       "      <td>1.082452</td>\n",
       "      <td>1.146555</td>\n",
       "      <td>1.090484</td>\n",
       "      <td>1.659797</td>\n",
       "      <td>2.571110</td>\n",
       "      <td>3.205332</td>\n",
       "      <td>3.410100</td>\n",
       "      <td>3.538129</td>\n",
       "      <td>3.616678</td>\n",
       "      <td>3.675339</td>\n",
       "      <td>3.682659</td>\n",
       "      <td>3.754580</td>\n",
       "      <td>3.710804</td>\n",
       "      <td>0.800777</td>\n",
       "      <td>0.901758</td>\n",
       "      <td>1.248479</td>\n",
       "      <td>1.495482</td>\n",
       "      <td>1.494265</td>\n",
       "      <td>2.381440</td>\n",
       "      <td>4.517328</td>\n",
       "      <td>4.982208</td>\n",
       "      <td>5.206562</td>\n",
       "      <td>5.262480</td>\n",
       "      <td>1_P</td>\n",
       "      <td>70.302656</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.164810</td>\n",
       "      <td>0.163459</td>\n",
       "      <td>0.161074</td>\n",
       "      <td>0.163151</td>\n",
       "      <td>0.166365</td>\n",
       "      <td>0.173361</td>\n",
       "      <td>0.176534</td>\n",
       "      <td>0.175363</td>\n",
       "      <td>0.174817</td>\n",
       "      <td>0.174266</td>\n",
       "      <td>0.175142</td>\n",
       "      <td>0.177567</td>\n",
       "      <td>0.179800</td>\n",
       "      <td>0.186686</td>\n",
       "      <td>0.194100</td>\n",
       "      <td>0.199420</td>\n",
       "      <td>0.207205</td>\n",
       "      <td>0.216406</td>\n",
       "      <td>0.226819</td>\n",
       "      <td>0.234233</td>\n",
       "      <td>0.245464</td>\n",
       "      <td>0.259351</td>\n",
       "      <td>0.275583</td>\n",
       "      <td>0.291327</td>\n",
       "      <td>0.300969</td>\n",
       "      <td>0.306700</td>\n",
       "      <td>0.313685</td>\n",
       "      <td>0.318030</td>\n",
       "      <td>0.323988</td>\n",
       "      <td>0.326964</td>\n",
       "      <td>0.330085</td>\n",
       "      <td>0.330050</td>\n",
       "      <td>0.327086</td>\n",
       "      <td>0.321105</td>\n",
       "      <td>0.313493</td>\n",
       "      <td>0.306747</td>\n",
       "      <td>0.303655</td>\n",
       "      <td>0.299919</td>\n",
       "      <td>0.298985</td>\n",
       "      <td>0.299455</td>\n",
       "      <td>0.300853</td>\n",
       "      <td>0.300812</td>\n",
       "      <td>0.300627</td>\n",
       "      <td>0.300261</td>\n",
       "      <td>0.301276</td>\n",
       "      <td>0.301984</td>\n",
       "      <td>0.300389</td>\n",
       "      <td>0.296479</td>\n",
       "      <td>0.293648</td>\n",
       "      <td>0.291786</td>\n",
       "      <td>...</td>\n",
       "      <td>2.470337</td>\n",
       "      <td>2.807123</td>\n",
       "      <td>2.898035</td>\n",
       "      <td>3.001448</td>\n",
       "      <td>3.048183</td>\n",
       "      <td>3.074964</td>\n",
       "      <td>3.106142</td>\n",
       "      <td>3.131777</td>\n",
       "      <td>3.165942</td>\n",
       "      <td>3.161512</td>\n",
       "      <td>3.161910</td>\n",
       "      <td>3.150329</td>\n",
       "      <td>3.199207</td>\n",
       "      <td>3.225819</td>\n",
       "      <td>3.235964</td>\n",
       "      <td>3.238828</td>\n",
       "      <td>3.231759</td>\n",
       "      <td>0.464090</td>\n",
       "      <td>0.469189</td>\n",
       "      <td>0.501251</td>\n",
       "      <td>0.635263</td>\n",
       "      <td>0.894274</td>\n",
       "      <td>0.885812</td>\n",
       "      <td>0.848520</td>\n",
       "      <td>0.821781</td>\n",
       "      <td>0.808230</td>\n",
       "      <td>0.666904</td>\n",
       "      <td>1.421769</td>\n",
       "      <td>2.859085</td>\n",
       "      <td>3.925901</td>\n",
       "      <td>4.236832</td>\n",
       "      <td>4.356789</td>\n",
       "      <td>4.433868</td>\n",
       "      <td>4.480423</td>\n",
       "      <td>4.470493</td>\n",
       "      <td>4.564563</td>\n",
       "      <td>4.577605</td>\n",
       "      <td>0.653497</td>\n",
       "      <td>0.721019</td>\n",
       "      <td>1.169714</td>\n",
       "      <td>1.269090</td>\n",
       "      <td>0.967839</td>\n",
       "      <td>2.094396</td>\n",
       "      <td>5.501404</td>\n",
       "      <td>6.153006</td>\n",
       "      <td>6.340413</td>\n",
       "      <td>6.413906</td>\n",
       "      <td>2_P</td>\n",
       "      <td>70.302656</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.196746</td>\n",
       "      <td>0.195933</td>\n",
       "      <td>0.192789</td>\n",
       "      <td>0.195939</td>\n",
       "      <td>0.199449</td>\n",
       "      <td>0.207141</td>\n",
       "      <td>0.210958</td>\n",
       "      <td>0.209670</td>\n",
       "      <td>0.209044</td>\n",
       "      <td>0.208800</td>\n",
       "      <td>0.209746</td>\n",
       "      <td>0.212316</td>\n",
       "      <td>0.214074</td>\n",
       "      <td>0.221122</td>\n",
       "      <td>0.227590</td>\n",
       "      <td>0.231175</td>\n",
       "      <td>0.236460</td>\n",
       "      <td>0.242244</td>\n",
       "      <td>0.248283</td>\n",
       "      <td>0.249548</td>\n",
       "      <td>0.254676</td>\n",
       "      <td>0.263047</td>\n",
       "      <td>0.274811</td>\n",
       "      <td>0.286976</td>\n",
       "      <td>0.292836</td>\n",
       "      <td>0.295371</td>\n",
       "      <td>0.299582</td>\n",
       "      <td>0.301323</td>\n",
       "      <td>0.305262</td>\n",
       "      <td>0.307263</td>\n",
       "      <td>0.310883</td>\n",
       "      <td>0.312774</td>\n",
       "      <td>0.313215</td>\n",
       "      <td>0.311683</td>\n",
       "      <td>0.309021</td>\n",
       "      <td>0.307060</td>\n",
       "      <td>0.308388</td>\n",
       "      <td>0.308638</td>\n",
       "      <td>0.310459</td>\n",
       "      <td>0.313198</td>\n",
       "      <td>0.316359</td>\n",
       "      <td>0.317897</td>\n",
       "      <td>0.319637</td>\n",
       "      <td>0.321255</td>\n",
       "      <td>0.325305</td>\n",
       "      <td>0.329748</td>\n",
       "      <td>0.332382</td>\n",
       "      <td>0.332927</td>\n",
       "      <td>0.335776</td>\n",
       "      <td>0.339599</td>\n",
       "      <td>...</td>\n",
       "      <td>2.703383</td>\n",
       "      <td>3.003249</td>\n",
       "      <td>3.069666</td>\n",
       "      <td>3.164584</td>\n",
       "      <td>3.224090</td>\n",
       "      <td>3.256872</td>\n",
       "      <td>3.295791</td>\n",
       "      <td>3.327406</td>\n",
       "      <td>3.369497</td>\n",
       "      <td>3.370013</td>\n",
       "      <td>3.375948</td>\n",
       "      <td>3.355584</td>\n",
       "      <td>3.416114</td>\n",
       "      <td>3.432584</td>\n",
       "      <td>3.417126</td>\n",
       "      <td>3.402455</td>\n",
       "      <td>3.384514</td>\n",
       "      <td>0.554447</td>\n",
       "      <td>0.560629</td>\n",
       "      <td>0.601122</td>\n",
       "      <td>0.693900</td>\n",
       "      <td>0.847613</td>\n",
       "      <td>0.872456</td>\n",
       "      <td>0.904919</td>\n",
       "      <td>0.968680</td>\n",
       "      <td>1.048155</td>\n",
       "      <td>0.943619</td>\n",
       "      <td>1.740963</td>\n",
       "      <td>3.192912</td>\n",
       "      <td>4.208852</td>\n",
       "      <td>4.468006</td>\n",
       "      <td>4.617549</td>\n",
       "      <td>4.709755</td>\n",
       "      <td>4.776806</td>\n",
       "      <td>4.769443</td>\n",
       "      <td>4.855114</td>\n",
       "      <td>4.807250</td>\n",
       "      <td>0.776119</td>\n",
       "      <td>0.859227</td>\n",
       "      <td>1.135058</td>\n",
       "      <td>1.346569</td>\n",
       "      <td>1.292224</td>\n",
       "      <td>2.551541</td>\n",
       "      <td>5.902475</td>\n",
       "      <td>6.501725</td>\n",
       "      <td>6.768157</td>\n",
       "      <td>6.804711</td>\n",
       "      <td>3_P</td>\n",
       "      <td>70.302656</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.265508</td>\n",
       "      <td>0.266124</td>\n",
       "      <td>0.264145</td>\n",
       "      <td>0.268839</td>\n",
       "      <td>0.274781</td>\n",
       "      <td>0.286389</td>\n",
       "      <td>0.293448</td>\n",
       "      <td>0.294926</td>\n",
       "      <td>0.297448</td>\n",
       "      <td>0.300688</td>\n",
       "      <td>0.305430</td>\n",
       "      <td>0.311505</td>\n",
       "      <td>0.315879</td>\n",
       "      <td>0.326377</td>\n",
       "      <td>0.336736</td>\n",
       "      <td>0.342160</td>\n",
       "      <td>0.348259</td>\n",
       "      <td>0.354244</td>\n",
       "      <td>0.359493</td>\n",
       "      <td>0.358178</td>\n",
       "      <td>0.361110</td>\n",
       "      <td>0.369255</td>\n",
       "      <td>0.383011</td>\n",
       "      <td>0.398926</td>\n",
       "      <td>0.408103</td>\n",
       "      <td>0.413587</td>\n",
       "      <td>0.421894</td>\n",
       "      <td>0.427198</td>\n",
       "      <td>0.435065</td>\n",
       "      <td>0.440875</td>\n",
       "      <td>0.448609</td>\n",
       "      <td>0.454896</td>\n",
       "      <td>0.459735</td>\n",
       "      <td>0.462830</td>\n",
       "      <td>0.464428</td>\n",
       "      <td>0.467005</td>\n",
       "      <td>0.473852</td>\n",
       "      <td>0.478739</td>\n",
       "      <td>0.485062</td>\n",
       "      <td>0.491590</td>\n",
       "      <td>0.498045</td>\n",
       "      <td>0.501412</td>\n",
       "      <td>0.503662</td>\n",
       "      <td>0.505074</td>\n",
       "      <td>0.508959</td>\n",
       "      <td>0.513719</td>\n",
       "      <td>0.516320</td>\n",
       "      <td>0.516151</td>\n",
       "      <td>0.518781</td>\n",
       "      <td>0.522190</td>\n",
       "      <td>...</td>\n",
       "      <td>1.237768</td>\n",
       "      <td>1.259918</td>\n",
       "      <td>1.276671</td>\n",
       "      <td>1.309850</td>\n",
       "      <td>1.336913</td>\n",
       "      <td>1.353656</td>\n",
       "      <td>1.375640</td>\n",
       "      <td>1.393388</td>\n",
       "      <td>1.415845</td>\n",
       "      <td>1.412746</td>\n",
       "      <td>1.414951</td>\n",
       "      <td>1.403705</td>\n",
       "      <td>1.400144</td>\n",
       "      <td>1.400054</td>\n",
       "      <td>1.416377</td>\n",
       "      <td>1.433226</td>\n",
       "      <td>1.447285</td>\n",
       "      <td>0.749112</td>\n",
       "      <td>0.761695</td>\n",
       "      <td>0.874678</td>\n",
       "      <td>1.005579</td>\n",
       "      <td>1.187955</td>\n",
       "      <td>1.321515</td>\n",
       "      <td>1.421946</td>\n",
       "      <td>1.485836</td>\n",
       "      <td>1.534201</td>\n",
       "      <td>1.592607</td>\n",
       "      <td>1.771794</td>\n",
       "      <td>1.753143</td>\n",
       "      <td>1.777770</td>\n",
       "      <td>1.850363</td>\n",
       "      <td>1.917786</td>\n",
       "      <td>1.973324</td>\n",
       "      <td>2.004276</td>\n",
       "      <td>1.986687</td>\n",
       "      <td>1.982129</td>\n",
       "      <td>2.028058</td>\n",
       "      <td>1.050053</td>\n",
       "      <td>1.222868</td>\n",
       "      <td>1.668053</td>\n",
       "      <td>2.016291</td>\n",
       "      <td>2.163232</td>\n",
       "      <td>2.443648</td>\n",
       "      <td>2.528219</td>\n",
       "      <td>2.715599</td>\n",
       "      <td>2.822610</td>\n",
       "      <td>2.819261</td>\n",
       "      <td>4_P</td>\n",
       "      <td>70.302656</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2403 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      feat1     feat2     feat3     feat4     feat5     feat6     feat7  \\\n",
       "0  0.173565  0.171952  0.168504  0.170999  0.173946  0.181525  0.185026   \n",
       "1  0.202796  0.201845  0.199020  0.202500  0.206132  0.214863  0.219335   \n",
       "2  0.164810  0.163459  0.161074  0.163151  0.166365  0.173361  0.176534   \n",
       "3  0.196746  0.195933  0.192789  0.195939  0.199449  0.207141  0.210958   \n",
       "4  0.265508  0.266124  0.264145  0.268839  0.274781  0.286389  0.293448   \n",
       "\n",
       "      feat8     feat9    feat10    feat11    feat12    feat13    feat14  \\\n",
       "0  0.184056  0.183710  0.183698  0.184985  0.187467  0.189658  0.196452   \n",
       "1  0.218668  0.218889  0.219440  0.221122  0.224898  0.227561  0.236100   \n",
       "2  0.175363  0.174817  0.174266  0.175142  0.177567  0.179800  0.186686   \n",
       "3  0.209670  0.209044  0.208800  0.209746  0.212316  0.214074  0.221122   \n",
       "4  0.294926  0.297448  0.300688  0.305430  0.311505  0.315879  0.326377   \n",
       "\n",
       "     feat15    feat16    feat17    feat18    feat19    feat20    feat21  \\\n",
       "0  0.202846  0.207412  0.213938  0.222476  0.232073  0.238128  0.247958   \n",
       "1  0.243862  0.248254  0.253956  0.260094  0.266156  0.267235  0.272653   \n",
       "2  0.194100  0.199420  0.207205  0.216406  0.226819  0.234233  0.245464   \n",
       "3  0.227590  0.231175  0.236460  0.242244  0.248283  0.249548  0.254676   \n",
       "4  0.336736  0.342160  0.348259  0.354244  0.359493  0.358178  0.361110   \n",
       "\n",
       "     feat22    feat23    feat24    feat25    feat26    feat27    feat28  \\\n",
       "0  0.260854  0.276417  0.291528  0.300351  0.305525  0.311848  0.315623   \n",
       "1  0.281744  0.294286  0.307994  0.314845  0.318900  0.324794  0.327503   \n",
       "2  0.259351  0.275583  0.291327  0.300969  0.306700  0.313685  0.318030   \n",
       "3  0.263047  0.274811  0.286976  0.292836  0.295371  0.299582  0.301323   \n",
       "4  0.369255  0.383011  0.398926  0.408103  0.413587  0.421894  0.427198   \n",
       "\n",
       "     feat29    feat30    feat31    feat32    feat33    feat34    feat35  \\\n",
       "0  0.320844  0.323720  0.327251  0.327792  0.325899  0.320993  0.314128   \n",
       "1  0.332724  0.336211  0.341693  0.345040  0.347407  0.347685  0.346067   \n",
       "2  0.323988  0.326964  0.330085  0.330050  0.327086  0.321105  0.313493   \n",
       "3  0.305262  0.307263  0.310883  0.312774  0.313215  0.311683  0.309021   \n",
       "4  0.435065  0.440875  0.448609  0.454896  0.459735  0.462830  0.464428   \n",
       "\n",
       "     feat36    feat37    feat38    feat39    feat40    feat41    feat42  \\\n",
       "0  0.308204  0.305358  0.301816  0.300822  0.301477  0.303310  0.303769   \n",
       "1  0.344727  0.346363  0.347024  0.349843  0.353515  0.357605  0.359821   \n",
       "2  0.306747  0.303655  0.299919  0.298985  0.299455  0.300853  0.300812   \n",
       "3  0.307060  0.308388  0.308638  0.310459  0.313198  0.316359  0.317897   \n",
       "4  0.467005  0.473852  0.478739  0.485062  0.491590  0.498045  0.501412   \n",
       "\n",
       "     feat43    feat44    feat45    feat46    feat47    feat48    feat49  \\\n",
       "0  0.304400  0.304489  0.306138  0.307097  0.305769  0.302423  0.300566   \n",
       "1  0.362217  0.363633  0.367676  0.371853  0.374272  0.374649  0.376807   \n",
       "2  0.300627  0.300261  0.301276  0.301984  0.300389  0.296479  0.293648   \n",
       "3  0.319637  0.321255  0.325305  0.329748  0.332382  0.332927  0.335776   \n",
       "4  0.503662  0.505074  0.508959  0.513719  0.516320  0.516151  0.518781   \n",
       "\n",
       "     feat50  ...  feat2354  feat2355  feat2356  feat2357  feat2358  feat2359  \\\n",
       "0  0.299315  ...  2.042916  2.297939  2.377649  2.466454  2.507972  2.530190   \n",
       "1  0.379905  ...  2.096276  2.280492  2.336384  2.416731  2.466844  2.495732   \n",
       "2  0.291786  ...  2.470337  2.807123  2.898035  3.001448  3.048183  3.074964   \n",
       "3  0.339599  ...  2.703383  3.003249  3.069666  3.164584  3.224090  3.256872   \n",
       "4  0.522190  ...  1.237768  1.259918  1.276671  1.309850  1.336913  1.353656   \n",
       "\n",
       "   feat2360  feat2361  feat2362  feat2363  feat2364  feat2365  feat2366  \\\n",
       "0  2.559515  2.581921  2.615295  2.617831  2.625087  2.613590  2.660765   \n",
       "1  2.528185  2.554744  2.590548  2.593327  2.601961  2.589237  2.643366   \n",
       "2  3.106142  3.131777  3.165942  3.161512  3.161910  3.150329  3.199207   \n",
       "3  3.295791  3.327406  3.369497  3.370013  3.375948  3.355584  3.416114   \n",
       "4  1.375640  1.393388  1.415845  1.412746  1.414951  1.403705  1.400144   \n",
       "\n",
       "   feat2367  feat2368  feat2369  feat2370  feat2371  feat2372  feat2373  \\\n",
       "0  2.680289  2.683594  2.687329  2.684616  0.489267  0.491960  0.528080   \n",
       "1  2.655447  2.637913  2.626337  2.615737  0.571762  0.577881  0.635002   \n",
       "2  3.225819  3.235964  3.238828  3.231759  0.464090  0.469189  0.501251   \n",
       "3  3.432584  3.417126  3.402455  3.384514  0.554447  0.560629  0.601122   \n",
       "4  1.400054  1.416377  1.433226  1.447285  0.749112  0.761695  0.874678   \n",
       "\n",
       "   feat2374  feat2375  feat2376  feat2377  feat2378  feat2379  feat2380  \\\n",
       "0  0.650628  0.887739  0.887057  0.858053  0.846928  0.843302  0.756928   \n",
       "1  0.744523  0.917573  0.976870  1.023253  1.082452  1.146555  1.090484   \n",
       "2  0.635263  0.894274  0.885812  0.848520  0.821781  0.808230  0.666904   \n",
       "3  0.693900  0.847613  0.872456  0.904919  0.968680  1.048155  0.943619   \n",
       "4  1.005579  1.187955  1.321515  1.421946  1.485836  1.534201  1.592607   \n",
       "\n",
       "   feat2381  feat2382  feat2383  feat2384  feat2385  feat2386  feat2387  \\\n",
       "0  1.354611  2.418203  3.218510  3.481309  3.586095  3.656175  3.709706   \n",
       "1  1.659797  2.571110  3.205332  3.410100  3.538129  3.616678  3.675339   \n",
       "2  1.421769  2.859085  3.925901  4.236832  4.356789  4.433868  4.480423   \n",
       "3  1.740963  3.192912  4.208852  4.468006  4.617549  4.709755  4.776806   \n",
       "4  1.771794  1.753143  1.777770  1.850363  1.917786  1.973324  2.004276   \n",
       "\n",
       "   feat2388  feat2389  feat2390  feat2391  feat2392  feat2393  feat2394  \\\n",
       "0  3.713505  3.791356  3.797913  0.688768  0.754274  1.174493  1.269823   \n",
       "1  3.682659  3.754580  3.710804  0.800777  0.901758  1.248479  1.495482   \n",
       "2  4.470493  4.564563  4.577605  0.653497  0.721019  1.169714  1.269090   \n",
       "3  4.769443  4.855114  4.807250  0.776119  0.859227  1.135058  1.346569   \n",
       "4  1.986687  1.982129  2.028058  1.050053  1.222868  1.668053  2.016291   \n",
       "\n",
       "   feat2395  feat2396  feat2397  feat2398  feat2399  feat2400  Target  \\\n",
       "0  1.064050  1.963784  4.521961  5.065660  5.246762  5.330273     0_P   \n",
       "1  1.494265  2.381440  4.517328  4.982208  5.206562  5.262480     1_P   \n",
       "2  0.967839  2.094396  5.501404  6.153006  6.340413  6.413906     2_P   \n",
       "3  1.292224  2.551541  5.902475  6.501725  6.768157  6.804711     3_P   \n",
       "4  2.163232  2.443648  2.528219  2.715599  2.822610  2.819261     4_P   \n",
       "\n",
       "   mean_norm  Param  \n",
       "0  70.302656      P  \n",
       "1  70.302656      P  \n",
       "2  70.302656      P  \n",
       "3  70.302656      P  \n",
       "4  70.302656      P  \n",
       "\n",
       "[5 rows x 2403 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['Param'] = test['Target'].str.split('_').str.get(1)\n",
    "train['Param'] = train['Target'].str.split('_').str.get(1)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CUKQ-KqSweoD",
    "outputId": "b49c9c73-bd80-4be5-fcb5-3d4fbefeebec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Param\n",
       "K     2020\n",
       "Mg    1983\n",
       "P     2420\n",
       "pH    1823\n",
       "Name: Value, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby(['Param'])['Value'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale Columns??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Value</th>\n",
       "      <th>feat1</th>\n",
       "      <th>feat2</th>\n",
       "      <th>feat3</th>\n",
       "      <th>feat4</th>\n",
       "      <th>feat5</th>\n",
       "      <th>feat6</th>\n",
       "      <th>feat7</th>\n",
       "      <th>feat8</th>\n",
       "      <th>feat9</th>\n",
       "      <th>feat10</th>\n",
       "      <th>feat11</th>\n",
       "      <th>feat12</th>\n",
       "      <th>feat13</th>\n",
       "      <th>feat14</th>\n",
       "      <th>feat15</th>\n",
       "      <th>feat16</th>\n",
       "      <th>feat17</th>\n",
       "      <th>feat18</th>\n",
       "      <th>feat19</th>\n",
       "      <th>feat20</th>\n",
       "      <th>feat21</th>\n",
       "      <th>feat22</th>\n",
       "      <th>feat23</th>\n",
       "      <th>feat24</th>\n",
       "      <th>feat25</th>\n",
       "      <th>feat26</th>\n",
       "      <th>feat27</th>\n",
       "      <th>feat28</th>\n",
       "      <th>feat29</th>\n",
       "      <th>feat30</th>\n",
       "      <th>feat31</th>\n",
       "      <th>feat32</th>\n",
       "      <th>feat33</th>\n",
       "      <th>feat34</th>\n",
       "      <th>feat35</th>\n",
       "      <th>feat36</th>\n",
       "      <th>feat37</th>\n",
       "      <th>feat38</th>\n",
       "      <th>feat39</th>\n",
       "      <th>feat40</th>\n",
       "      <th>feat41</th>\n",
       "      <th>feat42</th>\n",
       "      <th>feat43</th>\n",
       "      <th>feat44</th>\n",
       "      <th>feat45</th>\n",
       "      <th>feat46</th>\n",
       "      <th>feat47</th>\n",
       "      <th>feat48</th>\n",
       "      <th>...</th>\n",
       "      <th>feat2352</th>\n",
       "      <th>feat2353</th>\n",
       "      <th>feat2354</th>\n",
       "      <th>feat2355</th>\n",
       "      <th>feat2356</th>\n",
       "      <th>feat2357</th>\n",
       "      <th>feat2358</th>\n",
       "      <th>feat2359</th>\n",
       "      <th>feat2360</th>\n",
       "      <th>feat2361</th>\n",
       "      <th>feat2362</th>\n",
       "      <th>feat2363</th>\n",
       "      <th>feat2364</th>\n",
       "      <th>feat2365</th>\n",
       "      <th>feat2366</th>\n",
       "      <th>feat2367</th>\n",
       "      <th>feat2368</th>\n",
       "      <th>feat2369</th>\n",
       "      <th>feat2370</th>\n",
       "      <th>feat2371</th>\n",
       "      <th>feat2372</th>\n",
       "      <th>feat2373</th>\n",
       "      <th>feat2374</th>\n",
       "      <th>feat2375</th>\n",
       "      <th>feat2376</th>\n",
       "      <th>feat2377</th>\n",
       "      <th>feat2378</th>\n",
       "      <th>feat2379</th>\n",
       "      <th>feat2380</th>\n",
       "      <th>feat2381</th>\n",
       "      <th>feat2382</th>\n",
       "      <th>feat2383</th>\n",
       "      <th>feat2384</th>\n",
       "      <th>feat2385</th>\n",
       "      <th>feat2386</th>\n",
       "      <th>feat2387</th>\n",
       "      <th>feat2388</th>\n",
       "      <th>feat2389</th>\n",
       "      <th>feat2390</th>\n",
       "      <th>feat2391</th>\n",
       "      <th>feat2392</th>\n",
       "      <th>feat2393</th>\n",
       "      <th>feat2394</th>\n",
       "      <th>feat2395</th>\n",
       "      <th>feat2396</th>\n",
       "      <th>feat2397</th>\n",
       "      <th>feat2398</th>\n",
       "      <th>feat2399</th>\n",
       "      <th>feat2400</th>\n",
       "      <th>Param</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_P</td>\n",
       "      <td>45.1</td>\n",
       "      <td>0.203683</td>\n",
       "      <td>0.203992</td>\n",
       "      <td>0.202177</td>\n",
       "      <td>0.205636</td>\n",
       "      <td>0.210894</td>\n",
       "      <td>0.220505</td>\n",
       "      <td>0.226073</td>\n",
       "      <td>0.227012</td>\n",
       "      <td>0.228581</td>\n",
       "      <td>0.230678</td>\n",
       "      <td>0.233982</td>\n",
       "      <td>0.238588</td>\n",
       "      <td>0.241646</td>\n",
       "      <td>0.250014</td>\n",
       "      <td>0.258405</td>\n",
       "      <td>0.261899</td>\n",
       "      <td>0.266396</td>\n",
       "      <td>0.270737</td>\n",
       "      <td>0.274890</td>\n",
       "      <td>0.273968</td>\n",
       "      <td>0.276430</td>\n",
       "      <td>0.283327</td>\n",
       "      <td>0.295028</td>\n",
       "      <td>0.308494</td>\n",
       "      <td>0.316209</td>\n",
       "      <td>0.320717</td>\n",
       "      <td>0.327333</td>\n",
       "      <td>0.331691</td>\n",
       "      <td>0.337992</td>\n",
       "      <td>0.342740</td>\n",
       "      <td>0.349642</td>\n",
       "      <td>0.355668</td>\n",
       "      <td>0.360805</td>\n",
       "      <td>0.364883</td>\n",
       "      <td>0.367570</td>\n",
       "      <td>0.370319</td>\n",
       "      <td>0.376717</td>\n",
       "      <td>0.381299</td>\n",
       "      <td>0.387101</td>\n",
       "      <td>0.393264</td>\n",
       "      <td>0.399336</td>\n",
       "      <td>0.402669</td>\n",
       "      <td>0.405688</td>\n",
       "      <td>0.407629</td>\n",
       "      <td>0.411576</td>\n",
       "      <td>0.416467</td>\n",
       "      <td>0.419428</td>\n",
       "      <td>0.419526</td>\n",
       "      <td>...</td>\n",
       "      <td>1.095705</td>\n",
       "      <td>1.129028</td>\n",
       "      <td>1.153727</td>\n",
       "      <td>1.191501</td>\n",
       "      <td>1.206518</td>\n",
       "      <td>1.240862</td>\n",
       "      <td>1.266370</td>\n",
       "      <td>1.281632</td>\n",
       "      <td>1.303487</td>\n",
       "      <td>1.320699</td>\n",
       "      <td>1.343543</td>\n",
       "      <td>1.349235</td>\n",
       "      <td>1.355927</td>\n",
       "      <td>1.355282</td>\n",
       "      <td>1.360762</td>\n",
       "      <td>1.357333</td>\n",
       "      <td>1.372652</td>\n",
       "      <td>1.387570</td>\n",
       "      <td>1.400359</td>\n",
       "      <td>0.574390</td>\n",
       "      <td>0.584549</td>\n",
       "      <td>0.670901</td>\n",
       "      <td>0.769619</td>\n",
       "      <td>0.920841</td>\n",
       "      <td>1.042641</td>\n",
       "      <td>1.146396</td>\n",
       "      <td>1.211927</td>\n",
       "      <td>1.273028</td>\n",
       "      <td>1.312649</td>\n",
       "      <td>1.513034</td>\n",
       "      <td>1.593172</td>\n",
       "      <td>1.678123</td>\n",
       "      <td>1.751818</td>\n",
       "      <td>1.816820</td>\n",
       "      <td>1.870275</td>\n",
       "      <td>1.911317</td>\n",
       "      <td>1.919118</td>\n",
       "      <td>1.924227</td>\n",
       "      <td>1.962802</td>\n",
       "      <td>0.805311</td>\n",
       "      <td>0.936756</td>\n",
       "      <td>1.290456</td>\n",
       "      <td>1.626534</td>\n",
       "      <td>1.777969</td>\n",
       "      <td>2.094892</td>\n",
       "      <td>2.382082</td>\n",
       "      <td>2.567942</td>\n",
       "      <td>2.697733</td>\n",
       "      <td>2.733829</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_P</td>\n",
       "      <td>44.8</td>\n",
       "      <td>0.250603</td>\n",
       "      <td>0.249785</td>\n",
       "      <td>0.246786</td>\n",
       "      <td>0.250632</td>\n",
       "      <td>0.255035</td>\n",
       "      <td>0.265170</td>\n",
       "      <td>0.270217</td>\n",
       "      <td>0.269648</td>\n",
       "      <td>0.269498</td>\n",
       "      <td>0.270211</td>\n",
       "      <td>0.272503</td>\n",
       "      <td>0.276662</td>\n",
       "      <td>0.279905</td>\n",
       "      <td>0.288868</td>\n",
       "      <td>0.297204</td>\n",
       "      <td>0.301549</td>\n",
       "      <td>0.306996</td>\n",
       "      <td>0.313169</td>\n",
       "      <td>0.319590</td>\n",
       "      <td>0.320913</td>\n",
       "      <td>0.326546</td>\n",
       "      <td>0.336216</td>\n",
       "      <td>0.350023</td>\n",
       "      <td>0.364886</td>\n",
       "      <td>0.372914</td>\n",
       "      <td>0.377352</td>\n",
       "      <td>0.383670</td>\n",
       "      <td>0.387081</td>\n",
       "      <td>0.392847</td>\n",
       "      <td>0.396461</td>\n",
       "      <td>0.401595</td>\n",
       "      <td>0.405064</td>\n",
       "      <td>0.406857</td>\n",
       "      <td>0.406138</td>\n",
       "      <td>0.403846</td>\n",
       "      <td>0.402338</td>\n",
       "      <td>0.404513</td>\n",
       "      <td>0.405366</td>\n",
       "      <td>0.408354</td>\n",
       "      <td>0.412235</td>\n",
       "      <td>0.416498</td>\n",
       "      <td>0.418540</td>\n",
       "      <td>0.420223</td>\n",
       "      <td>0.421406</td>\n",
       "      <td>0.424962</td>\n",
       "      <td>0.428652</td>\n",
       "      <td>0.430207</td>\n",
       "      <td>0.429708</td>\n",
       "      <td>...</td>\n",
       "      <td>1.502146</td>\n",
       "      <td>1.758616</td>\n",
       "      <td>2.011209</td>\n",
       "      <td>2.160844</td>\n",
       "      <td>2.194579</td>\n",
       "      <td>2.249652</td>\n",
       "      <td>2.285842</td>\n",
       "      <td>2.307155</td>\n",
       "      <td>2.331671</td>\n",
       "      <td>2.351600</td>\n",
       "      <td>2.378405</td>\n",
       "      <td>2.375424</td>\n",
       "      <td>2.375780</td>\n",
       "      <td>2.363581</td>\n",
       "      <td>2.404702</td>\n",
       "      <td>2.419109</td>\n",
       "      <td>2.419744</td>\n",
       "      <td>2.425575</td>\n",
       "      <td>2.429126</td>\n",
       "      <td>0.706865</td>\n",
       "      <td>0.714469</td>\n",
       "      <td>0.781618</td>\n",
       "      <td>0.895926</td>\n",
       "      <td>1.084361</td>\n",
       "      <td>1.143641</td>\n",
       "      <td>1.187382</td>\n",
       "      <td>1.231635</td>\n",
       "      <td>1.280067</td>\n",
       "      <td>1.232515</td>\n",
       "      <td>1.738577</td>\n",
       "      <td>2.511333</td>\n",
       "      <td>3.038777</td>\n",
       "      <td>3.175525</td>\n",
       "      <td>3.269756</td>\n",
       "      <td>3.328903</td>\n",
       "      <td>3.365557</td>\n",
       "      <td>3.357815</td>\n",
       "      <td>3.421406</td>\n",
       "      <td>3.428380</td>\n",
       "      <td>0.992393</td>\n",
       "      <td>1.105412</td>\n",
       "      <td>1.483055</td>\n",
       "      <td>1.721859</td>\n",
       "      <td>1.699395</td>\n",
       "      <td>2.483675</td>\n",
       "      <td>4.277375</td>\n",
       "      <td>4.606743</td>\n",
       "      <td>4.764096</td>\n",
       "      <td>4.809254</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2_P</td>\n",
       "      <td>44.4</td>\n",
       "      <td>0.191200</td>\n",
       "      <td>0.189831</td>\n",
       "      <td>0.187725</td>\n",
       "      <td>0.191629</td>\n",
       "      <td>0.196252</td>\n",
       "      <td>0.205140</td>\n",
       "      <td>0.209914</td>\n",
       "      <td>0.210256</td>\n",
       "      <td>0.210999</td>\n",
       "      <td>0.211794</td>\n",
       "      <td>0.214068</td>\n",
       "      <td>0.218355</td>\n",
       "      <td>0.221650</td>\n",
       "      <td>0.229133</td>\n",
       "      <td>0.235590</td>\n",
       "      <td>0.238543</td>\n",
       "      <td>0.242081</td>\n",
       "      <td>0.245498</td>\n",
       "      <td>0.248445</td>\n",
       "      <td>0.247065</td>\n",
       "      <td>0.249292</td>\n",
       "      <td>0.254473</td>\n",
       "      <td>0.263325</td>\n",
       "      <td>0.273994</td>\n",
       "      <td>0.279563</td>\n",
       "      <td>0.283124</td>\n",
       "      <td>0.288288</td>\n",
       "      <td>0.291484</td>\n",
       "      <td>0.296931</td>\n",
       "      <td>0.301143</td>\n",
       "      <td>0.306834</td>\n",
       "      <td>0.311672</td>\n",
       "      <td>0.315907</td>\n",
       "      <td>0.318279</td>\n",
       "      <td>0.319126</td>\n",
       "      <td>0.320443</td>\n",
       "      <td>0.324220</td>\n",
       "      <td>0.326946</td>\n",
       "      <td>0.331071</td>\n",
       "      <td>0.335602</td>\n",
       "      <td>0.340231</td>\n",
       "      <td>0.343282</td>\n",
       "      <td>0.345376</td>\n",
       "      <td>0.346862</td>\n",
       "      <td>0.350116</td>\n",
       "      <td>0.353510</td>\n",
       "      <td>0.355157</td>\n",
       "      <td>0.355238</td>\n",
       "      <td>...</td>\n",
       "      <td>0.868626</td>\n",
       "      <td>0.886784</td>\n",
       "      <td>0.897235</td>\n",
       "      <td>0.921281</td>\n",
       "      <td>0.942381</td>\n",
       "      <td>0.972106</td>\n",
       "      <td>0.993024</td>\n",
       "      <td>1.004513</td>\n",
       "      <td>1.016845</td>\n",
       "      <td>1.026514</td>\n",
       "      <td>1.040119</td>\n",
       "      <td>1.028430</td>\n",
       "      <td>1.024554</td>\n",
       "      <td>1.021568</td>\n",
       "      <td>1.045355</td>\n",
       "      <td>1.059899</td>\n",
       "      <td>1.062368</td>\n",
       "      <td>1.069155</td>\n",
       "      <td>1.073775</td>\n",
       "      <td>0.539424</td>\n",
       "      <td>0.545641</td>\n",
       "      <td>0.616830</td>\n",
       "      <td>0.695538</td>\n",
       "      <td>0.813433</td>\n",
       "      <td>0.904103</td>\n",
       "      <td>0.975803</td>\n",
       "      <td>1.019546</td>\n",
       "      <td>1.059941</td>\n",
       "      <td>1.061241</td>\n",
       "      <td>1.211937</td>\n",
       "      <td>1.250024</td>\n",
       "      <td>1.300998</td>\n",
       "      <td>1.373126</td>\n",
       "      <td>1.422106</td>\n",
       "      <td>1.455332</td>\n",
       "      <td>1.458446</td>\n",
       "      <td>1.450438</td>\n",
       "      <td>1.497318</td>\n",
       "      <td>1.510940</td>\n",
       "      <td>0.756372</td>\n",
       "      <td>0.862336</td>\n",
       "      <td>1.140623</td>\n",
       "      <td>1.386935</td>\n",
       "      <td>1.473936</td>\n",
       "      <td>1.671600</td>\n",
       "      <td>1.852783</td>\n",
       "      <td>2.015047</td>\n",
       "      <td>2.058019</td>\n",
       "      <td>2.104777</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 2403 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Target  Value     feat1     feat2     feat3     feat4     feat5     feat6  \\\n",
       "0    0_P   45.1  0.203683  0.203992  0.202177  0.205636  0.210894  0.220505   \n",
       "1    1_P   44.8  0.250603  0.249785  0.246786  0.250632  0.255035  0.265170   \n",
       "2    2_P   44.4  0.191200  0.189831  0.187725  0.191629  0.196252  0.205140   \n",
       "\n",
       "      feat7     feat8     feat9    feat10    feat11    feat12    feat13  \\\n",
       "0  0.226073  0.227012  0.228581  0.230678  0.233982  0.238588  0.241646   \n",
       "1  0.270217  0.269648  0.269498  0.270211  0.272503  0.276662  0.279905   \n",
       "2  0.209914  0.210256  0.210999  0.211794  0.214068  0.218355  0.221650   \n",
       "\n",
       "     feat14    feat15    feat16    feat17    feat18    feat19    feat20  \\\n",
       "0  0.250014  0.258405  0.261899  0.266396  0.270737  0.274890  0.273968   \n",
       "1  0.288868  0.297204  0.301549  0.306996  0.313169  0.319590  0.320913   \n",
       "2  0.229133  0.235590  0.238543  0.242081  0.245498  0.248445  0.247065   \n",
       "\n",
       "     feat21    feat22    feat23    feat24    feat25    feat26    feat27  \\\n",
       "0  0.276430  0.283327  0.295028  0.308494  0.316209  0.320717  0.327333   \n",
       "1  0.326546  0.336216  0.350023  0.364886  0.372914  0.377352  0.383670   \n",
       "2  0.249292  0.254473  0.263325  0.273994  0.279563  0.283124  0.288288   \n",
       "\n",
       "     feat28    feat29    feat30    feat31    feat32    feat33    feat34  \\\n",
       "0  0.331691  0.337992  0.342740  0.349642  0.355668  0.360805  0.364883   \n",
       "1  0.387081  0.392847  0.396461  0.401595  0.405064  0.406857  0.406138   \n",
       "2  0.291484  0.296931  0.301143  0.306834  0.311672  0.315907  0.318279   \n",
       "\n",
       "     feat35    feat36    feat37    feat38    feat39    feat40    feat41  \\\n",
       "0  0.367570  0.370319  0.376717  0.381299  0.387101  0.393264  0.399336   \n",
       "1  0.403846  0.402338  0.404513  0.405366  0.408354  0.412235  0.416498   \n",
       "2  0.319126  0.320443  0.324220  0.326946  0.331071  0.335602  0.340231   \n",
       "\n",
       "     feat42    feat43    feat44    feat45    feat46    feat47    feat48  ...  \\\n",
       "0  0.402669  0.405688  0.407629  0.411576  0.416467  0.419428  0.419526  ...   \n",
       "1  0.418540  0.420223  0.421406  0.424962  0.428652  0.430207  0.429708  ...   \n",
       "2  0.343282  0.345376  0.346862  0.350116  0.353510  0.355157  0.355238  ...   \n",
       "\n",
       "   feat2352  feat2353  feat2354  feat2355  feat2356  feat2357  feat2358  \\\n",
       "0  1.095705  1.129028  1.153727  1.191501  1.206518  1.240862  1.266370   \n",
       "1  1.502146  1.758616  2.011209  2.160844  2.194579  2.249652  2.285842   \n",
       "2  0.868626  0.886784  0.897235  0.921281  0.942381  0.972106  0.993024   \n",
       "\n",
       "   feat2359  feat2360  feat2361  feat2362  feat2363  feat2364  feat2365  \\\n",
       "0  1.281632  1.303487  1.320699  1.343543  1.349235  1.355927  1.355282   \n",
       "1  2.307155  2.331671  2.351600  2.378405  2.375424  2.375780  2.363581   \n",
       "2  1.004513  1.016845  1.026514  1.040119  1.028430  1.024554  1.021568   \n",
       "\n",
       "   feat2366  feat2367  feat2368  feat2369  feat2370  feat2371  feat2372  \\\n",
       "0  1.360762  1.357333  1.372652  1.387570  1.400359  0.574390  0.584549   \n",
       "1  2.404702  2.419109  2.419744  2.425575  2.429126  0.706865  0.714469   \n",
       "2  1.045355  1.059899  1.062368  1.069155  1.073775  0.539424  0.545641   \n",
       "\n",
       "   feat2373  feat2374  feat2375  feat2376  feat2377  feat2378  feat2379  \\\n",
       "0  0.670901  0.769619  0.920841  1.042641  1.146396  1.211927  1.273028   \n",
       "1  0.781618  0.895926  1.084361  1.143641  1.187382  1.231635  1.280067   \n",
       "2  0.616830  0.695538  0.813433  0.904103  0.975803  1.019546  1.059941   \n",
       "\n",
       "   feat2380  feat2381  feat2382  feat2383  feat2384  feat2385  feat2386  \\\n",
       "0  1.312649  1.513034  1.593172  1.678123  1.751818  1.816820  1.870275   \n",
       "1  1.232515  1.738577  2.511333  3.038777  3.175525  3.269756  3.328903   \n",
       "2  1.061241  1.211937  1.250024  1.300998  1.373126  1.422106  1.455332   \n",
       "\n",
       "   feat2387  feat2388  feat2389  feat2390  feat2391  feat2392  feat2393  \\\n",
       "0  1.911317  1.919118  1.924227  1.962802  0.805311  0.936756  1.290456   \n",
       "1  3.365557  3.357815  3.421406  3.428380  0.992393  1.105412  1.483055   \n",
       "2  1.458446  1.450438  1.497318  1.510940  0.756372  0.862336  1.140623   \n",
       "\n",
       "   feat2394  feat2395  feat2396  feat2397  feat2398  feat2399  feat2400  Param  \n",
       "0  1.626534  1.777969  2.094892  2.382082  2.567942  2.697733  2.733829      P  \n",
       "1  1.721859  1.699395  2.483675  4.277375  4.606743  4.764096  4.809254      P  \n",
       "2  1.386935  1.473936  1.671600  1.852783  2.015047  2.058019  2.104777      P  \n",
       "\n",
       "[3 rows x 2403 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode the 'period_of_day' column\n",
    "#combined_df['period_of_day'] = label_encoder.fit_transform(combined_df['period_of_day'])\n",
    "\n",
    "#scale columns\n",
    "def scale_columns(df):\n",
    "    scaler = MinMaxScaler()\n",
    "    #scaler = StandardScaler()\n",
    "    #scaler = RobustScaler()\n",
    "    cols_to_exclude = [\"Target\",\"mean_norm\",\"Param\",\"Value\"] \n",
    "    cols_to_scale = [col for col in df.columns if col not in cols_to_exclude]\n",
    "    scaled_values = scaler.fit_transform(df[cols_to_scale])\n",
    "    scaled_df = df.copy()\n",
    "    scaled_df[cols_to_scale] = scaled_values\n",
    "    return scaled_df\n",
    "#train= scale_columns(train)\n",
    "#test=scale_columns(test)\n",
    "train.head(3)#\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Most correlated??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Special_cols1 = ['Target','Value', 'Param']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"%%time\\nnum_feature = [col for col in train.columns if col not in Special_cols1]\\ndrop_columns=[]\\ncorr = train[num_feature].corr()\\n# Drop highly correlated features\\ncolumns = np.full((corr.shape[0],), True, dtype=bool)\\n\\nfor i in range(corr.shape[0]):\\n    for j in range(i+1, corr.shape[0]):\\n        if corr.iloc[i,j] >=0.95 :\\n            if columns[j]:\\n                columns[j] = False\\n                print('FEAT_A: {} FEAT_B: {} - Correlation: {}'.format(train[num_feature].columns[i] , train[num_feature].columns[j], corr.iloc[i,j]))\\n        elif corr.iloc[i,j] <= -0.95:\\n            if columns[j]:\\n                columns[j] = False\\n\\ndrop_columns = train[num_feature].columns[columns == False].values\\nprint('drop_columns',len(drop_columns),drop_columns)\\n\\ntrain.drop(drop_columns,inplace=True,axis =1)\\nprint(train.shape)\\ntest.drop(drop_columns,inplace=True,axis =1)\\nprint(test.shape)\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"%%time\n",
    "num_feature = [col for col in train.columns if col not in Special_cols1]\n",
    "drop_columns=[]\n",
    "corr = train[num_feature].corr()\n",
    "# Drop highly correlated features\n",
    "columns = np.full((corr.shape[0],), True, dtype=bool)\n",
    "\n",
    "for i in range(corr.shape[0]):\n",
    "    for j in range(i+1, corr.shape[0]):\n",
    "        if corr.iloc[i,j] >=0.95 :\n",
    "            if columns[j]:\n",
    "                columns[j] = False\n",
    "                print('FEAT_A: {} FEAT_B: {} - Correlation: {}'.format(train[num_feature].columns[i] , train[num_feature].columns[j], corr.iloc[i,j]))\n",
    "        elif corr.iloc[i,j] <= -0.95:\n",
    "            if columns[j]:\n",
    "                columns[j] = False\n",
    "\n",
    "drop_columns = train[num_feature].columns[columns == False].values\n",
    "print('drop_columns',len(drop_columns),drop_columns)\n",
    "\n",
    "train.drop(drop_columns,inplace=True,axis =1)\n",
    "print(train.shape)\n",
    "test.drop(drop_columns,inplace=True,axis =1)\n",
    "print(test.shape)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['feat1', 'feat2', 'feat3', 'feat4', 'feat5', 'feat6', 'feat7', 'feat8',\n",
       "       'feat9', 'feat10',\n",
       "       ...\n",
       "       'feat2394', 'feat2395', 'feat2396', 'feat2397', 'feat2398', 'feat2399',\n",
       "       'feat2400', 'Target', 'mean_norm', 'Param'],\n",
       "      dtype='object', length=2403)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Important Features Only?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/abhishekkrthakur/approachingalmost\n",
    "from sklearn.feature_selection import f_regression, mutual_info_regression, SelectPercentile\n",
    "class UnivariateFeatureSelction:\n",
    "    def __init__(self, n_features, problem_type, scoring, return_cols=True):\n",
    "        \"\"\"\n",
    "        Custom univariate feature selection wrapper on\n",
    "        different univariate feature selection models from\n",
    "        scikit-learn.\n",
    "        :param n_features: SelectPercentile if float else SelectKBest\n",
    "        :param problem_type: classification or regression\n",
    "        :param scoring: scoring function, string\n",
    "        \"\"\"\n",
    "        self.n_features = n_features\n",
    "\n",
    "        if problem_type == \"classification\":\n",
    "            valid_scoring = {\n",
    "                \"f_classif\": f_classif,\n",
    "                \"chi2\": chi2,\n",
    "                \"mutual_info_classif\": mutual_info_classif\n",
    "            }\n",
    "        else:\n",
    "            valid_scoring = {\n",
    "                \"f_regression\": f_regression,\n",
    "                \"mutual_info_regression\": mutual_info_regression\n",
    "            }\n",
    "        if scoring not in valid_scoring:\n",
    "            raise Exception(\"Invalid scoring function\")\n",
    "\n",
    "        if isinstance(n_features, int):\n",
    "            self.selection = SelectKBest(\n",
    "                valid_scoring[scoring],\n",
    "                k=n_features\n",
    "            )\n",
    "        elif isinstance(n_features, float):\n",
    "            self.selection = SelectPercentile(\n",
    "                valid_scoring[scoring],\n",
    "                percentile=int(n_features * 100)\n",
    "            )\n",
    "        else:\n",
    "            raise Exception(\"Invalid type of feature\")\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self.selection.fit(X, y)\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.selection.transform(X)\n",
    "\n",
    "    def fit_transform(self, X, y):\n",
    "        return self.selection.fit_transform(X, y)\n",
    "\n",
    "    def return_cols(self, X):\n",
    "        if isinstance(self.n_features, int):\n",
    "            mask = SelectKBest.get_support(self.selection)\n",
    "            selected_features = []\n",
    "            features = list(X.columns)\n",
    "            for bool, feature in zip(mask, features):\n",
    "                if bool:\n",
    "                    selected_features.append(feature)\n",
    "\n",
    "        elif isinstance(self.n_features, float):\n",
    "            mask = SelectPercentile.get_support(self.selection)\n",
    "            selected_features = []\n",
    "            features = list(X.columns)\n",
    "            for bool, feature in zip(mask, features):\n",
    "                if bool:\n",
    "                    selected_features.append(feature)\n",
    "        else:\n",
    "            raise Exception(\"Invalid type of feature\")\n",
    "\n",
    "        return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_train = [\n",
    "    train[train['Param'] == \"P\"],\n",
    "    train[train['Param'] == \"K\"],\n",
    "    train[train['Param'] == \"Mg\"],\n",
    "    train[train['Param'] == \"pH\"],\n",
    "]\n",
    "\n",
    "datasets_test = [\n",
    "    test[test['Param'] == \"P\"],\n",
    "    test[test['Param'] == \"K\"],\n",
    "    test[test['Param'] == \"Mg\"],\n",
    "    test[test['Param'] == \"pH\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vve3lhiDDJdl"
   },
   "source": [
    "# BASELINE MODEL PERFORMANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selected_features_dict = {}  # Create an empty dictionary to store selected features for each dataset\n",
    "dataset=train[train['Param'] == \"P\"]\n",
    "X = dataset.drop(columns=[\"Target\", \"Param\", \"Value\"])\n",
    "y = dataset[\"Value\"]\n",
    "\n",
    "\"\"\"ufs = UnivariateFeatureSelction(\n",
    "    n_features=0.9,  # Select 90% of the most important features\n",
    "    problem_type=\"regression\",\n",
    "    scoring=\"f_regression\"\n",
    ")\n",
    "\n",
    "ufs.fit(X, y.values.ravel())\n",
    "selected_features = ufs.return_cols(X)\"\"\"\n",
    "selected_features = X.columns\n",
    "#print(f\"Selected features for P: {selected_features}\")\n",
    "\n",
    "#selected_features_dict[dataset_name] = selected_features  # Store selected features in the dictionary\n",
    "\n",
    "# Now selected_features_dict contains selected features for each dataset\n",
    "# You can access them using dataset names as keys\n",
    "X=X[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model for Dataset 0: HistGradientBoostingRegressor(random_state=1234)\n",
      "Results for Dataset 0:\n",
      "â•’â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â••\n",
      "â”‚    â”‚ Model    â”‚    RMSE â”‚\n",
      "â•žâ•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚  0 â”‚ LGBMoost â”‚ 22.2547 â”‚\n",
      "â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚  1 â”‚ LGBMoost â”‚ 15.1639 â”‚\n",
      "â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚  2 â”‚ LGBMoost â”‚ 14.4393 â”‚\n",
      "â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚  3 â”‚ LGBMoost â”‚ 17.7383 â”‚\n",
      "â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚  4 â”‚ LGBMoost â”‚ 13.2555 â”‚\n",
      "â•˜â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•›\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate_model(X_train, y_train, X_test, y_test, selected_columns=None):\n",
    "    if selected_columns is not None:\n",
    "        # If selected columns are provided, use only those columns\n",
    "        X_train = X_train[selected_columns]\n",
    "        X_test = X_test[selected_columns]\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    # (Rest of the code remains unchanged)\n",
    "    model = HistGradientBoostingRegressor(random_state=1234, verbose=0)\n",
    "    model_name = \"LGBMoost\"\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    rmse = mean_squared_error(y_test, preds, squared=False)\n",
    "\n",
    "    return model, model_name, rmse\n",
    "\n",
    "best_model = None\n",
    "best_rmse = float('inf')\n",
    "results_per_dataset = []\n",
    "best_models_per_dataset = []\n",
    "# Initialize a KFold with the number of splits\n",
    "n_splits = 5  # You can adjust this as needed\n",
    "kfold = KFold(n_splits=n_splits)\n",
    "results = []\n",
    "for train_idx, test_idx in kfold.split(X):\n",
    "    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "    X_test, y_test = X.iloc[test_idx], y.iloc[test_idx]\n",
    "\n",
    "    model, model_name, rmse = train_and_evaluate_model(X_train, y_train, X_test, y_test, selected_features)\n",
    "    results.append([model_name, rmse])\n",
    "\n",
    "    # Check if this model has a lower RMSE than the best model so far\n",
    "    if rmse < best_rmse:\n",
    "        best_model = model\n",
    "        best_rmse = rmse\n",
    "\n",
    "results_per_dataset.append(results)\n",
    "best_models_per_dataset.append(best_model)\n",
    "\n",
    "# Print results for each dataset\n",
    "for i, best_model in enumerate(best_models_per_dataset):\n",
    "    print(f\"Best Model for Dataset {i}: {best_model}\")\n",
    "\n",
    "# Print results for each dataset\n",
    "for i, results in enumerate(results_per_dataset):\n",
    "    print(f\"Results for Dataset {i}:\")\n",
    "    results_df = pd.DataFrame(results, columns=[\"Model\", \"RMSE\"])\n",
    "    print(tabulate(results_df, headers=\"keys\", tablefmt=\"fancy_grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_index</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_P</td>\n",
       "      <td>1.144516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_P</td>\n",
       "      <td>1.077322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2_P</td>\n",
       "      <td>0.896702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3_P</td>\n",
       "      <td>1.102193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4_P</td>\n",
       "      <td>0.853410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sample_index    Target\n",
       "0          0_P  1.144516\n",
       "1          1_P  1.077322\n",
       "2          2_P  0.896702\n",
       "3          3_P  1.102193\n",
       "4          4_P  0.853410"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions on the Zindi test set\n",
    "model=best_models_per_dataset[0]\n",
    "test_df = datasets_test[0][selected_features]\n",
    "preds = model.predict(test_df) #.fillna(0)\n",
    "\n",
    "# Create submisiion file to be uploaded to Zindi for scoring\n",
    "sub_k = pd.DataFrame({'sample_index': datasets_test[0]['Target'], 'Target': preds})\n",
    "sub_k['Target']=sub_k['Target']/70.3026558891455\n",
    "sub_k.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selected_features_dict = {}  # Create an empty dictionary to store selected features for each dataset\n",
    "dataset=train[train['Param'] == \"K\"]\n",
    "X = dataset.drop(columns=[\"Target\", \"Param\", \"Value\"])\n",
    "y = dataset[\"Value\"]\n",
    "\"\"\"\n",
    "ufs = UnivariateFeatureSelction(\n",
    "    n_features=0.9,  # Select 90% of the most important features\n",
    "    problem_type=\"regression\",\n",
    "    scoring=\"f_regression\"\n",
    ")\n",
    "\n",
    "ufs.fit(X, y.values.ravel())\n",
    "selected_features = ufs.return_cols(X)\"\"\"\n",
    "selected_features = X.columns\n",
    "\n",
    "#print(f\"Selected features for P: {selected_features}\")\n",
    "\n",
    "#selected_features_dict[dataset_name] = selected_features  # Store selected features in the dictionary\n",
    "\n",
    "# Now selected_features_dict contains selected features for each dataset\n",
    "# You can access them using dataset names as keys\n",
    "X=X[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model for Dataset 0: HistGradientBoostingRegressor(random_state=1234)\n",
      "Results for Dataset 0:\n",
      "â•’â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â••\n",
      "â”‚    â”‚ Model    â”‚    RMSE â”‚\n",
      "â•žâ•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚  0 â”‚ LGBMoost â”‚ 33.5658 â”‚\n",
      "â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚  1 â”‚ LGBMoost â”‚ 40.9432 â”‚\n",
      "â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚  2 â”‚ LGBMoost â”‚ 27.4212 â”‚\n",
      "â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚  3 â”‚ LGBMoost â”‚ 39.9622 â”‚\n",
      "â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚  4 â”‚ LGBMoost â”‚ 35.598  â”‚\n",
      "â•˜â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•›\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate_model(X_train, y_train, X_test, y_test, selected_columns=None):\n",
    "    if selected_columns is not None:\n",
    "        # If selected columns are provided, use only those columns\n",
    "        X_train = X_train[selected_columns]\n",
    "        X_test = X_test[selected_columns]\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    # (Rest of the code remains unchanged)\n",
    "    model = HistGradientBoostingRegressor(random_state=1234, verbose=0)\n",
    "    model_name = \"LGBMoost\"\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    rmse = mean_squared_error(y_test, preds, squared=False)\n",
    "\n",
    "    return model, model_name, rmse\n",
    "\n",
    "best_model = None\n",
    "best_rmse = float('inf')\n",
    "results_per_dataset = []\n",
    "best_models_per_dataset = []\n",
    "# Initialize a KFold with the number of splits\n",
    "n_splits = 5  # You can adjust this as needed\n",
    "kfold = KFold(n_splits=n_splits)\n",
    "results = []\n",
    "for train_idx, test_idx in kfold.split(X):\n",
    "    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "    X_test, y_test = X.iloc[test_idx], y.iloc[test_idx]\n",
    "\n",
    "    model, model_name, rmse = train_and_evaluate_model(X_train, y_train, X_test, y_test, selected_features)\n",
    "    results.append([model_name, rmse])\n",
    "\n",
    "    # Check if this model has a lower RMSE than the best model so far\n",
    "    if rmse < best_rmse:\n",
    "        best_model = model\n",
    "        best_rmse = rmse\n",
    "\n",
    "results_per_dataset.append(results)\n",
    "best_models_per_dataset.append(best_model)\n",
    "\n",
    "# Print results for each dataset\n",
    "for i, best_model in enumerate(best_models_per_dataset):\n",
    "    print(f\"Best Model for Dataset {i}: {best_model}\")\n",
    "\n",
    "# Print results for each dataset\n",
    "for i, results in enumerate(results_per_dataset):\n",
    "    print(f\"Results for Dataset {i}:\")\n",
    "    results_df = pd.DataFrame(results, columns=[\"Model\", \"RMSE\"])\n",
    "    print(tabulate(results_df, headers=\"keys\", tablefmt=\"fancy_grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_index</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1154</th>\n",
       "      <td>0_K</td>\n",
       "      <td>0.891158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155</th>\n",
       "      <td>1_K</td>\n",
       "      <td>0.988521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156</th>\n",
       "      <td>2_K</td>\n",
       "      <td>1.055790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1157</th>\n",
       "      <td>3_K</td>\n",
       "      <td>1.015227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1158</th>\n",
       "      <td>4_K</td>\n",
       "      <td>0.905135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sample_index    Target\n",
       "1154          0_K  0.891158\n",
       "1155          1_K  0.988521\n",
       "1156          2_K  1.055790\n",
       "1157          3_K  1.015227\n",
       "1158          4_K  0.905135"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions on the Zindi test set\n",
    "model=best_models_per_dataset[0]\n",
    "test_df = datasets_test[1][selected_features]\n",
    "preds = model.predict(test_df) #.fillna(1)\n",
    "# Create submisiion file to be uploaded to Zindi for scoring\n",
    "sub_p = pd.DataFrame({'sample_index': datasets_test[1]['Target'], 'Target': preds})\n",
    "sub_p['Target']=sub_p['Target']/227.9885103926097\n",
    "#sub_p['Target']=sub_p['Target']/234.18273333\n",
    "sub_p.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selected_features_dict = {}  # Create an empty dictionary to store selected features for each dataset\n",
    "dataset=train[train['Param'] == \"Mg\"]\n",
    "X = dataset.drop(columns=[\"Target\", \"Param\", \"Value\"])\n",
    "y = dataset[\"Value\"]\n",
    "\n",
    "\"\"\"ufs = UnivariateFeatureSelction(\n",
    "    n_features=0.9,  # Select 90% of the most important features\n",
    "    problem_type=\"regression\",\n",
    "    scoring=\"f_regression\"\n",
    ")\n",
    "\n",
    "ufs.fit(X, y.values.ravel())\n",
    "selected_features = ufs.return_cols(X)\"\"\"\n",
    "\n",
    "selected_features = X.columns\n",
    "#print(f\"Selected features for P: {selected_features}\")\n",
    "\n",
    "#selected_features_dict[dataset_name] = selected_features  # Store selected features in the dictionary\n",
    "\n",
    "# Now selected_features_dict contains selected features for each dataset\n",
    "# You can access them using dataset names as keys\n",
    "X=X[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model for Dataset 0: HistGradientBoostingRegressor(random_state=1234)\n",
      "Results for Dataset 0:\n",
      "â•’â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â••\n",
      "â”‚    â”‚ Model    â”‚    RMSE â”‚\n",
      "â•žâ•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚  0 â”‚ LGBMoost â”‚ 29.8413 â”‚\n",
      "â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚  1 â”‚ LGBMoost â”‚ 14.6883 â”‚\n",
      "â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚  2 â”‚ LGBMoost â”‚ 22.7145 â”‚\n",
      "â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚  3 â”‚ LGBMoost â”‚ 18.4412 â”‚\n",
      "â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚  4 â”‚ LGBMoost â”‚ 15.2905 â”‚\n",
      "â•˜â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•›\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate_model(X_train, y_train, X_test, y_test, selected_columns=None):\n",
    "    if selected_columns is not None:\n",
    "        # If selected columns are provided, use only those columns\n",
    "        X_train = X_train[selected_columns]\n",
    "        X_test = X_test[selected_columns]\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    # (Rest of the code remains unchanged)\n",
    "    model = HistGradientBoostingRegressor(random_state=1234, verbose=0)\n",
    "    model_name = \"LGBMoost\"\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    rmse = mean_squared_error(y_test, preds, squared=False)\n",
    "\n",
    "    return model, model_name, rmse\n",
    "\n",
    "best_model = None\n",
    "best_rmse = float('inf')\n",
    "results_per_dataset = []\n",
    "best_models_per_dataset = []\n",
    "# Initialize a KFold with the number of splits\n",
    "n_splits = 5  # You can adjust this as needed\n",
    "kfold = KFold(n_splits=n_splits)\n",
    "results = []\n",
    "for train_idx, test_idx in kfold.split(X):\n",
    "    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "    X_test, y_test = X.iloc[test_idx], y.iloc[test_idx]\n",
    "\n",
    "    model, model_name, rmse = train_and_evaluate_model(X_train, y_train, X_test, y_test, selected_features)\n",
    "    results.append([model_name, rmse])\n",
    "\n",
    "    # Check if this model has a lower RMSE than the best model so far\n",
    "    if rmse < best_rmse:\n",
    "        best_model = model\n",
    "        best_rmse = rmse\n",
    "\n",
    "results_per_dataset.append(results)\n",
    "best_models_per_dataset.append(best_model)\n",
    "\n",
    "# Print results for each dataset\n",
    "for i, best_model in enumerate(best_models_per_dataset):\n",
    "    print(f\"Best Model for Dataset {i}: {best_model}\")\n",
    "\n",
    "# Print results for each dataset\n",
    "for i, results in enumerate(results_per_dataset):\n",
    "    print(f\"Results for Dataset {i}:\")\n",
    "    results_df = pd.DataFrame(results, columns=[\"Model\", \"RMSE\"])\n",
    "    print(tabulate(results_df, headers=\"keys\", tablefmt=\"fancy_grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_index</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2308</th>\n",
       "      <td>0_Mg</td>\n",
       "      <td>0.762736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2309</th>\n",
       "      <td>1_Mg</td>\n",
       "      <td>0.933107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2310</th>\n",
       "      <td>2_Mg</td>\n",
       "      <td>1.095319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2311</th>\n",
       "      <td>3_Mg</td>\n",
       "      <td>1.193688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2312</th>\n",
       "      <td>4_Mg</td>\n",
       "      <td>1.004106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sample_index    Target\n",
       "2308         0_Mg  0.762736\n",
       "2309         1_Mg  0.933107\n",
       "2310         2_Mg  1.095319\n",
       "2311         3_Mg  1.193688\n",
       "2312         4_Mg  1.004106"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions on the Zindi test set\n",
    "model=best_models_per_dataset[0]\n",
    "test_df = datasets_test[2][selected_features]\n",
    "preds = model.predict(test_df) #.fillna(0)\n",
    "\n",
    "# Create submisiion file to be uploaded to Zindi for scoring\n",
    "sub_mg = pd.DataFrame({'sample_index': datasets_test[2]['Target'], 'Target': preds})\n",
    "sub_mg['Target']=sub_mg['Target']/159.28123556581986\n",
    "sub_mg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selected_features_dict = {}  # Create an empty dictionary to store selected features for each dataset\n",
    "dataset=train[train['Param'] == \"pH\"]\n",
    "X = dataset.drop(columns=[\"Target\", \"Param\", \"Value\"])\n",
    "y = dataset[\"Value\"]\n",
    "\n",
    "\"\"\"ufs = UnivariateFeatureSelction(\n",
    "    n_features=0.9,  # Select 90% of the most important features\n",
    "    problem_type=\"regression\",\n",
    "    scoring=\"f_regression\"\n",
    ")\n",
    "\n",
    "ufs.fit(X, y.values.ravel())\n",
    "selected_features = ufs.return_cols(X)\"\"\"\n",
    "\n",
    "selected_features = X.columns\n",
    "\n",
    "#print(f\"Selected features for P: {selected_features}\")\n",
    "\n",
    "#selected_features_dict[dataset_name] = selected_features  # Store selected features in the dictionary\n",
    "\n",
    "# Now selected_features_dict contains selected features for each dataset\n",
    "# You can access them using dataset names as keys\n",
    "X=X[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model for Dataset 0: HistGradientBoostingRegressor(random_state=1234)\n",
      "Results for Dataset 0:\n",
      "â•’â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â••\n",
      "â”‚    â”‚ Model    â”‚     RMSE â”‚\n",
      "â•žâ•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚  0 â”‚ LGBMoost â”‚ 0.16585  â”‚\n",
      "â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚  1 â”‚ LGBMoost â”‚ 0.142672 â”‚\n",
      "â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚  2 â”‚ LGBMoost â”‚ 0.131732 â”‚\n",
      "â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚  3 â”‚ LGBMoost â”‚ 0.141275 â”‚\n",
      "â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚  4 â”‚ LGBMoost â”‚ 0.135748 â”‚\n",
      "â•˜â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•›\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate_model(X_train, y_train, X_test, y_test, selected_columns=None):\n",
    "    if selected_columns is not None:\n",
    "        # If selected columns are provided, use only those columns\n",
    "        X_train = X_train[selected_columns]\n",
    "        X_test = X_test[selected_columns]\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    # (Rest of the code remains unchanged)\n",
    "    model = HistGradientBoostingRegressor(random_state=1234, verbose=0)\n",
    "    model_name = \"LGBMoost\"\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    rmse = mean_squared_error(y_test, preds, squared=False)\n",
    "\n",
    "    return model, model_name, rmse\n",
    "\n",
    "best_model = None\n",
    "best_rmse = float('inf')\n",
    "results_per_dataset = []\n",
    "best_models_per_dataset = []\n",
    "# Initialize a KFold with the number of splits\n",
    "n_splits = 5  # You can adjust this as needed\n",
    "kfold = KFold(n_splits=n_splits)\n",
    "results = []\n",
    "for train_idx, test_idx in kfold.split(X):\n",
    "    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "    X_test, y_test = X.iloc[test_idx], y.iloc[test_idx]\n",
    "\n",
    "    model, model_name, rmse = train_and_evaluate_model(X_train, y_train, X_test, y_test, selected_features)\n",
    "    results.append([model_name, rmse])\n",
    "\n",
    "    # Check if this model has a lower RMSE than the best model so far\n",
    "    if rmse < best_rmse:\n",
    "        best_model = model\n",
    "        best_rmse = rmse\n",
    "\n",
    "results_per_dataset.append(results)\n",
    "best_models_per_dataset.append(best_model)\n",
    "\n",
    "# Print results for each dataset\n",
    "for i, best_model in enumerate(best_models_per_dataset):\n",
    "    print(f\"Best Model for Dataset {i}: {best_model}\")\n",
    "\n",
    "# Print results for each dataset\n",
    "for i, results in enumerate(results_per_dataset):\n",
    "    print(f\"Results for Dataset {i}:\")\n",
    "    results_df = pd.DataFrame(results, columns=[\"Model\", \"RMSE\"])\n",
    "    print(tabulate(results_df, headers=\"keys\", tablefmt=\"fancy_grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_index</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3462</th>\n",
       "      <td>0_pH</td>\n",
       "      <td>1.017651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3463</th>\n",
       "      <td>1_pH</td>\n",
       "      <td>1.014242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3464</th>\n",
       "      <td>2_pH</td>\n",
       "      <td>1.004131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3465</th>\n",
       "      <td>3_pH</td>\n",
       "      <td>1.004610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3466</th>\n",
       "      <td>4_pH</td>\n",
       "      <td>1.010353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sample_index    Target\n",
       "3462         0_pH  1.017651\n",
       "3463         1_pH  1.014242\n",
       "3464         2_pH  1.004131\n",
       "3465         3_pH  1.004610\n",
       "3466         4_pH  1.010353"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions on the Zindi test set\n",
    "model=best_models_per_dataset[0]\n",
    "test_df = datasets_test[3][selected_features]\n",
    "preds = model.predict(test_df) #.fillna(0)\n",
    "\n",
    "# Create submisiion file to be uploaded to Zindi for scoring\n",
    "sub_ph = pd.DataFrame({'sample_index': datasets_test[3]['Target'], 'Target': preds})\n",
    "sub_ph['Target']=sub_ph['Target']/6.782719399538106\n",
    "sub_ph.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "-n1-bbkhhmVv",
    "outputId": "86a85ab7-166c-4cdd-cb45-6d310d3bb9cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4616, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_index</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1154</th>\n",
       "      <td>0_K</td>\n",
       "      <td>0.891158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155</th>\n",
       "      <td>1_K</td>\n",
       "      <td>0.988521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156</th>\n",
       "      <td>2_K</td>\n",
       "      <td>1.055790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1157</th>\n",
       "      <td>3_K</td>\n",
       "      <td>1.015227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1158</th>\n",
       "      <td>4_K</td>\n",
       "      <td>0.905135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4611</th>\n",
       "      <td>1149_pH</td>\n",
       "      <td>0.971884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4612</th>\n",
       "      <td>1150_pH</td>\n",
       "      <td>0.959529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4613</th>\n",
       "      <td>1151_pH</td>\n",
       "      <td>0.972270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4614</th>\n",
       "      <td>1152_pH</td>\n",
       "      <td>0.963777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4615</th>\n",
       "      <td>1153_pH</td>\n",
       "      <td>0.967716</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4616 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sample_index    Target\n",
       "1154          0_K  0.891158\n",
       "1155          1_K  0.988521\n",
       "1156          2_K  1.055790\n",
       "1157          3_K  1.015227\n",
       "1158          4_K  0.905135\n",
       "...           ...       ...\n",
       "4611      1149_pH  0.971884\n",
       "4612      1150_pH  0.959529\n",
       "4613      1151_pH  0.972270\n",
       "4614      1152_pH  0.963777\n",
       "4615      1153_pH  0.967716\n",
       "\n",
       "[4616 rows x 2 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub=pd.concat([sub_p,sub_k,sub_mg,sub_ph])\n",
    "sub.to_csv('HIST.csv', index = False)\n",
    "print(sub.shape)\n",
    "sub#.head()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
